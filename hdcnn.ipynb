{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import division, print_function, unicode_literals, absolute_import\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# To make make consistent across code blocks\n",
    "rnd.seed(42)\n",
    "\n",
    "# Ploting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Preprocessing functions\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Saving Parameters\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"data\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To Normalized\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function shifts image data to be centered around 0 and bounded\n",
    "#        by negative one and one.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array containing image data of range [0, 255]\n",
    "# \n",
    "#    Returns:\n",
    "#        Same array centered around 0 and bounded by [-1, 1]\n",
    "################################################################################\n",
    "def to_normalized(X):\n",
    "    X = (X-128)/128\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To One Hot\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function encodes label data with one hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Get Cifar Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function gets the Cifar dataset\n",
    "# \n",
    "#    Returns:\n",
    "#        Arrays containing training, testing, and validation data\n",
    "################################################################################\n",
    "def get_cifar_dataset():\n",
    "    # Import data set\n",
    "    from keras.datasets import cifar100\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # Adjust image data to center around 0 and be bound by [-1, 1]\n",
    "    x_train = to_normalized(x_train)\n",
    "    x_test = to_normalized(x_test)\n",
    "\n",
    "    # Encode labels as one hot\n",
    "    y_train = to_one_hot(y_train)\n",
    "    y_test  = to_one_hot(y_test)\n",
    "    \n",
    "    # Split training set into train and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=0)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Clip and Flip\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This clips images into the corners and center images then flips each\n",
    "#        such that the resulting concatenated set is 10x as large\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of image data\n",
    "#        Y    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def clip_n_flip(X, Y):\n",
    "    Y_conc = np.concatenate([Y, Y, Y, Y, Y, Y, Y, Y, Y, Y], axis=0)\n",
    "    X_a = X[:,:28,:28]\n",
    "    X_b = X[:,4:,:28]\n",
    "    X_c = X[:,:28,4:]\n",
    "    X_d = X[:,4:,4:]\n",
    "    X_e = X[:,2:30,2:30]\n",
    "    X_clip = np.concatenate([X_a,X_b,X_c,X_d,X_e], axis=0)\n",
    "    X_flip = np.concatenate([X_clip, np.fliplr(X_clip)], axis=0)\n",
    "    return X_flip, Y_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = get_cifar_dataset()\n",
    "\n",
    "# Clip and flip the images\n",
    "x_train, y_train = clip_n_flip(x_train, y_train)\n",
    "\n",
    "# Shuffle the images\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=1)\n",
    "\n",
    "# Clip the validation and test sets to match training set\n",
    "x_val = x_val[:,2:30,2:30]\n",
    "x_test = x_test[:,2:30,2:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Independent Layers\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function creates the set of layers that the coarse and each fine\n",
    "#        classifier will have.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            The last tensor in the shared set of tensors\n",
    "#        set_index    The index of the classifier section \n",
    "# \n",
    "#    Returns:\n",
    "#        Last tensor prior to the softmax layer\n",
    "################################################################################\n",
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(128, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    \n",
    "    net = Conv2D(256, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(4))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(5))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(6))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Flatten()(net)\n",
    "    \n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(7))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Concatenate, Dot, RepeatVector, Reshape, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "input_img = Input(shape=(28, 28, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(64, 5, strides=1, padding='same', activation='elu', name='shared_layer_1')(input_img)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_2')(net_shared)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_3')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "net = indep_layers(net_shared, '0')\n",
    "output_c = Dense(fine_categories, activation='softmax')(net)\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base_model.load_weights('data/models/base_model_70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "450000/450000 [==============================] - 202s - loss: 3.9508 - acc: 0.1024 - val_loss: 3.8586 - val_acc: 0.1390\n",
      "Epoch 2/5\n",
      "450000/450000 [==============================] - 202s - loss: 3.5118 - acc: 0.1663 - val_loss: 3.4114 - val_acc: 0.1908\n",
      "Epoch 3/5\n",
      "450000/450000 [==============================] - 202s - loss: 3.3500 - acc: 0.1920 - val_loss: 3.5127 - val_acc: 0.1798\n",
      "Epoch 4/5\n",
      "450000/450000 [==============================] - 202s - loss: 3.2417 - acc: 0.2098 - val_loss: 3.3313 - val_acc: 0.2054\n",
      "Epoch 5/5\n",
      "450000/450000 [==============================] - 202s - loss: 3.1685 - acc: 0.2228 - val_loss: 3.2027 - val_acc: 0.2308\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 6/10\n",
      "450000/450000 [==============================] - 202s - loss: 3.1113 - acc: 0.2330 - val_loss: 3.2591 - val_acc: 0.2130\n",
      "Epoch 7/10\n",
      "450000/450000 [==============================] - 201s - loss: 3.0626 - acc: 0.2409 - val_loss: 3.2599 - val_acc: 0.2154\n",
      "Epoch 8/10\n",
      "450000/450000 [==============================] - 202s - loss: 3.0193 - acc: 0.2472 - val_loss: 3.1740 - val_acc: 0.2358\n",
      "Epoch 9/10\n",
      "450000/450000 [==============================] - 202s - loss: 2.9828 - acc: 0.2548 - val_loss: 2.8792 - val_acc: 0.2762\n",
      "Epoch 10/10\n",
      "450000/450000 [==============================] - 202s - loss: 2.9484 - acc: 0.2607 - val_loss: 2.8397 - val_acc: 0.2878\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 11/15\n",
      "450000/450000 [==============================] - 202s - loss: 2.9196 - acc: 0.2661 - val_loss: 2.8549 - val_acc: 0.2862\n",
      "Epoch 12/15\n",
      "450000/450000 [==============================] - 202s - loss: 2.8921 - acc: 0.2709 - val_loss: 2.8051 - val_acc: 0.2866\n",
      "Epoch 13/15\n",
      "450000/450000 [==============================] - 202s - loss: 2.8689 - acc: 0.2756 - val_loss: 2.8070 - val_acc: 0.2976\n",
      "Epoch 14/15\n",
      "450000/450000 [==============================] - 202s - loss: 2.8480 - acc: 0.2797 - val_loss: 2.6900 - val_acc: 0.3152\n",
      "Epoch 15/15\n",
      "450000/450000 [==============================] - 202s - loss: 2.8300 - acc: 0.2830 - val_loss: 2.8085 - val_acc: 0.2978\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 16/20\n",
      "450000/450000 [==============================] - 202s - loss: 2.8090 - acc: 0.2868 - val_loss: 2.9001 - val_acc: 0.2902\n",
      "Epoch 17/20\n",
      "450000/450000 [==============================] - 202s - loss: 2.7913 - acc: 0.2901 - val_loss: 2.7051 - val_acc: 0.3140\n",
      "Epoch 18/20\n",
      "450000/450000 [==============================] - 202s - loss: 2.7786 - acc: 0.2918 - val_loss: 2.7983 - val_acc: 0.3086\n",
      "Epoch 19/20\n",
      "450000/450000 [==============================] - 202s - loss: 2.7646 - acc: 0.2951 - val_loss: 2.5932 - val_acc: 0.3420\n",
      "Epoch 20/20\n",
      "450000/450000 [==============================] - 202s - loss: 2.7516 - acc: 0.2980 - val_loss: 2.7218 - val_acc: 0.3152\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 21/25\n",
      "450000/450000 [==============================] - 202s - loss: 2.7411 - acc: 0.2997 - val_loss: 2.8287 - val_acc: 0.2978\n",
      "Epoch 22/25\n",
      "450000/450000 [==============================] - 202s - loss: 2.7246 - acc: 0.3024 - val_loss: 2.5471 - val_acc: 0.3462\n",
      "Epoch 23/25\n",
      "450000/450000 [==============================] - 202s - loss: 2.7167 - acc: 0.3032 - val_loss: 2.6527 - val_acc: 0.3232\n",
      "Epoch 24/25\n",
      "450000/450000 [==============================] - 202s - loss: 2.7036 - acc: 0.3069 - val_loss: 2.5058 - val_acc: 0.3552\n",
      "Epoch 25/25\n",
      "450000/450000 [==============================] - 202s - loss: 2.6942 - acc: 0.3088 - val_loss: 2.6015 - val_acc: 0.3410\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 26/30\n",
      "450000/450000 [==============================] - 202s - loss: 2.6862 - acc: 0.3087 - val_loss: 2.4853 - val_acc: 0.3688\n",
      "Epoch 27/30\n",
      "450000/450000 [==============================] - 202s - loss: 2.6789 - acc: 0.3107 - val_loss: 2.4891 - val_acc: 0.3604\n",
      "Epoch 28/30\n",
      "450000/450000 [==============================] - 201s - loss: 2.6705 - acc: 0.3123 - val_loss: 2.5677 - val_acc: 0.3384\n",
      "Epoch 29/30\n",
      "450000/450000 [==============================] - 202s - loss: 2.6623 - acc: 0.3138 - val_loss: 2.5480 - val_acc: 0.3510\n",
      "Epoch 30/30\n",
      "450000/450000 [==============================] - 202s - loss: 2.6528 - acc: 0.3165 - val_loss: 2.5716 - val_acc: 0.3566\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 31/35\n",
      "450000/450000 [==============================] - 202s - loss: 2.6473 - acc: 0.3174 - val_loss: 2.5850 - val_acc: 0.3442\n",
      "Epoch 32/35\n",
      "450000/450000 [==============================] - 202s - loss: 2.6394 - acc: 0.3190 - val_loss: 2.5956 - val_acc: 0.3428\n",
      "Epoch 33/35\n",
      "450000/450000 [==============================] - 202s - loss: 2.6345 - acc: 0.3191 - val_loss: 2.5176 - val_acc: 0.3592\n",
      "Epoch 34/35\n",
      "450000/450000 [==============================] - 202s - loss: 2.6269 - acc: 0.3213 - val_loss: 2.5080 - val_acc: 0.3616\n",
      "Epoch 35/35\n",
      "450000/450000 [==============================] - 201s - loss: 2.6229 - acc: 0.3219 - val_loss: 2.5151 - val_acc: 0.3568\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 36/40\n",
      "136704/450000 [========>.....................] - ETA: 140s - loss: 2.6125 - acc: 0.3246"
     ]
    }
   ],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 70\n",
    "\n",
    "while index < stop:\n",
    "    base_model.fit(x_train, y_train, initial_epoch=index, epochs=index+step, batch_size=256, validation_data=(x_val, y_val))\n",
    "    index += step\n",
    "    base_model.save_weights('data/models/base_model_'+str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "##### THRESHOLDING LAYER IS NOT FUNCTIONAL #####\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    This layer takes in a set of coarse probabilities and an\n",
    "    index pointing to the relevant coarse probability\n",
    "    \n",
    "    It then thresholds that probability following:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    \n",
    "    Finally it multiplies that thresholded value across the \n",
    "    full input layer.\n",
    "    \n",
    "    # Input shape\n",
    "        Arbitrary 4-D Tensor\n",
    "    # Output\n",
    "        An tensor with the same shape as the input but with values\n",
    "        that are either all 1s or all 0s\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, index, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "        self.index = index\n",
    "        self.prob = prob\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Get Shape of Input\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        # Pull out coarse probability as specified by self.prob\n",
    "        X = self.prob[:,self.index:self.index+1]\n",
    "        # Threshold probability\n",
    "        X = K.cast(K.greater(X, self.theta), K.floatx())\n",
    "        # Repeat and Reshape thresholded probability to match input\n",
    "        X = K.tile(X, [1, shape[1]*shape[2]*shape[3]])\n",
    "        X = K.reshape(X, [-1,shape[1],shape[2],shape[3]])\n",
    "        # Multiply input with thresholded probability\n",
    "        X = Multiply()([X,inputs])\n",
    "        return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Calculate Probabalistic Averaging Layer\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function calculates the coarse classifier probabilities and\n",
    "#        extends that to a probabalistic averaging over fine classifications\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X      The output of the coarse classifier\n",
    "# \n",
    "#    Returns:\n",
    "#        Coarse category predictions and probabalistic averaging layer\n",
    "################################################################################\n",
    "def calc_prob_ave(X):\n",
    "    # Coarse to Fine mapping\n",
    "    ck = Input(shape=(20, 100), dtype='float32', name='C2K')\n",
    "    \n",
    "    # Extend input dimension from (n,100) to (n,1,100)\n",
    "    X = RepeatVector(1)(X)\n",
    "    \n",
    "    # Dot input, (n,1,100) and coarse to fine mapping (n,20,100) on axis 2\n",
    "    X = Dot(axes=2)([ck,X])\n",
    "    # Flatten result to (n,20)\n",
    "    Y = Reshape([20])(X)\n",
    "    \n",
    "    # Dot the vectors again to get probabalistic averaging layer (n,20,100)*(n,20,1) = (n,1,100)\n",
    "    X = Dot(axes=1)([ck,X])\n",
    "    # Flatten the result to (n,100)\n",
    "    X = Reshape([100])(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
