{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# To make make consistent across code blocks\n",
    "rnd.seed(42)\n",
    "\n",
    "# Ploting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Saving Parameters\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"data\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_normalized(X):\n",
    "    X = (X-128)/128\n",
    "    \n",
    "\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_adj = to_one_hot(y_train)\n",
    "y_test_adj  = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(48, (4, 4), strides=(1, 1), padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "    net = Conv2D(64, (2, 2), strides=(1, 1), padding='same', activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Conv2D(64, (2, 2), strides=(1, 1), padding='same', activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = Flatten()(net)\n",
    "\n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(4))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(5))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Dense(100, activation='softmax')(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Dot, RepeatVector, Reshape\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "\n",
    "coarse_categories = 20\n",
    "fine_categories = 100\n",
    "sigma = .01\n",
    "\n",
    "input_img = Input(shape=(32, 32, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(8, (4, 4), strides=(1, 1), padding='same', activation='elu')(input_img)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "\n",
    "net_shared = Conv2D(16, (4, 4), strides=(1, 1), padding='same', activation='elu')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "\n",
    "net_shared = Conv2D(32, (4, 4), strides=(1, 1), padding='same', activation='elu')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "net_shared = Conv2D(32, (4, 4), strides=(1, 1), padding='same', activation='elu')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "\n",
    "net_shared = Conv2D(48, (4, 4), strides=(1, 1), padding='same', activation='elu')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "# Get the coarse predictions from fine\n",
    "output_c = indep_layers(net_shared, '0')\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/700\n",
      "45000/45000 [==============================] - 68s - loss: 4.7295 - acc: 0.0133 - val_loss: 4.5164 - val_acc: 0.0204\n",
      "Epoch 2/700\n",
      "45000/45000 [==============================] - 71s - loss: 4.4019 - acc: 0.0293 - val_loss: 4.3171 - val_acc: 0.0384\n",
      "Epoch 3/700\n",
      "45000/45000 [==============================] - 66s - loss: 4.2304 - acc: 0.0433 - val_loss: 4.2441 - val_acc: 0.0412\n",
      "Epoch 4/700\n",
      "45000/45000 [==============================] - 73s - loss: 4.1084 - acc: 0.0548 - val_loss: 4.0384 - val_acc: 0.0694\n",
      "Epoch 5/700\n",
      "45000/45000 [==============================] - 68s - loss: 3.9993 - acc: 0.0675 - val_loss: 4.0126 - val_acc: 0.0768\n",
      "Epoch 6/700\n",
      "26112/45000 [================>.............] - ETA: 29s - loss: 3.9258 - acc: 0.0770"
     ]
    }
   ],
   "source": [
    "num_epochs = 700\n",
    "base_model.fit(x_train, y_train_adj, initial_epoch=0, epochs=num_epochs, batch_size=256, validation_split=.1)\n",
    "base_model.save_weights('data/models/base_model_'+str(num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from keras.layers import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    It follows:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as the input.\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return K.cast(K.greater(inputs, self.theta), K.floatx())\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Threshold and Probabalistic Averaging Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_prob_ave(fine_pred, fine_to_coarse):\n",
    "    coarse_pred = Dot(1)([fine_pred,fine_to_coarse])\n",
    "    coarse_pred_adj = RepeatVector(100)(coarse_pred)\n",
    "    coarse_pred_adj = Flatten()(coarse_pred_adj)\n",
    "    prob_ave = Multiply()([fine_to_coarse, coarse_pred_adj])\n",
    "\n",
    "    return prob_ave\n",
    "\n",
    "def calc_threshold(fine_pred, fine_to_coarse, shared_layer):\n",
    "    coarse_pred = Dot(1)([fine_pred, fine_to_coarse])\n",
    "    coarse_pred_thresholded = Threshold()(coarse_pred)\n",
    "    threshold_adj = RepeatVector(4*4*48)(coarse_pred_thresholded)\n",
    "    threshold_adj = Reshape((4, 4, 48))(threshold_adj)\n",
    "    shared_layer_adj = Multiply()([shared_layer, threshold_adj])\n",
    "    \n",
    "    return shared_layer_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an array of inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fine_inputs = [Input(shape=([100]), dtype='float32', name='c0'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c1'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c2'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c3'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c4'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c5'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c6'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c7'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c8'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c9'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c10'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c11'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c12'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c13'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c14'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c15'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c16'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c17'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c18'), \\\n",
    "        Input(shape=([100]), dtype='float32', name='c19')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    thresholded = calc_threshold(base_model.output, fine_inputs[i], net_shared)\n",
    "    fine_output = indep_layers(thresholded, str(i+1))\n",
    "\n",
    "    prob_ave = calc_prob_ave(output_c, fine_inputs[i])\n",
    "\n",
    "    weighted = Multiply()([fine_output, prob_ave])\n",
    "    \n",
    "    if i==0: output_f = weighted\n",
    "    else: output_f = Add()([weighted, output_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = fine_inputs\n",
    "\n",
    "model = Model(inputs=[base_model.input, c[0], c[1], c[2], c[3], c[4], c[5], c[6], c[7], c[8], c[9], c[10], c[11], c[12], c[13], c[14], c[15], c[16], c[17], c[18], c[19]], \n",
    "              outputs=output_f)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer= SGD(lr=.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer_name = layer.get_config()['name']\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1\n",
    "\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#print(C2K_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "45000/45000 [==============================] - 187s - loss: 3.2167 - acc: 0.2125 - val_loss: 3.0190 - val_acc: 0.2702\n",
      "Epoch 2/5\n",
      "45000/45000 [==============================] - 167s - loss: 3.1322 - acc: 0.2278 - val_loss: 3.0155 - val_acc: 0.2712\n",
      "Epoch 3/5\n",
      "45000/45000 [==============================] - 169s - loss: 3.1103 - acc: 0.2322 - val_loss: 2.9976 - val_acc: 0.2756\n",
      "Epoch 4/5\n",
      "45000/45000 [==============================] - 167s - loss: 3.0875 - acc: 0.2390 - val_loss: 2.9833 - val_acc: 0.2744\n",
      "Epoch 5/5\n",
      "45000/45000 [==============================] - 168s - loss: 3.0724 - acc: 0.2394 - val_loss: 2.9798 - val_acc: 0.2836\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 168s - loss: 3.0626 - acc: 0.2433 - val_loss: 2.9902 - val_acc: 0.2726\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 168s - loss: 3.0631 - acc: 0.2450 - val_loss: 2.9906 - val_acc: 0.2746\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 168s - loss: 3.0523 - acc: 0.2470 - val_loss: 2.9909 - val_acc: 0.2712\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 167s - loss: 3.0512 - acc: 0.2481 - val_loss: 2.9821 - val_acc: 0.2718\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 168s - loss: 3.0499 - acc: 0.2487 - val_loss: 2.9848 - val_acc: 0.2778\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 11/15\n",
      "45000/45000 [==============================] - 169s - loss: 3.0398 - acc: 0.2493 - val_loss: 2.9889 - val_acc: 0.2782\n",
      "Epoch 12/15\n",
      "45000/45000 [==============================] - 167s - loss: 3.0496 - acc: 0.2480 - val_loss: 2.9918 - val_acc: 0.2720\n",
      "Epoch 13/15\n",
      "45000/45000 [==============================] - 169s - loss: 3.0420 - acc: 0.2487 - val_loss: 2.9715 - val_acc: 0.2784\n",
      "Epoch 14/15\n",
      "45000/45000 [==============================] - 168s - loss: 3.0438 - acc: 0.2521 - val_loss: 2.9901 - val_acc: 0.2744\n",
      "Epoch 15/15\n",
      "45000/45000 [==============================] - 168s - loss: 3.0481 - acc: 0.2460 - val_loss: 2.9877 - val_acc: 0.2744\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 167s - loss: 3.0295 - acc: 0.2513 - val_loss: 2.9819 - val_acc: 0.2766\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 168s - loss: 3.0484 - acc: 0.2506 - val_loss: 2.9899 - val_acc: 0.2786\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 178s - loss: 3.0412 - acc: 0.2511 - val_loss: 2.9754 - val_acc: 0.2782\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 168s - loss: 3.0444 - acc: 0.2506 - val_loss: 3.0010 - val_acc: 0.2750\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 168s - loss: 3.0393 - acc: 0.2522 - val_loss: 3.0139 - val_acc: 0.2704\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 21/25\n",
      "45000/45000 [==============================] - 167s - loss: 3.0436 - acc: 0.2510 - val_loss: 2.9867 - val_acc: 0.2702\n",
      "Epoch 22/25\n",
      "45000/45000 [==============================] - 167s - loss: 3.0503 - acc: 0.2514 - val_loss: 2.9865 - val_acc: 0.2742\n",
      "Epoch 23/25\n",
      "45000/45000 [==============================] - 167s - loss: 3.0452 - acc: 0.2497 - val_loss: 2.9869 - val_acc: 0.2786\n",
      "Epoch 24/25\n",
      "45000/45000 [==============================] - 169s - loss: 3.0535 - acc: 0.2507 - val_loss: 3.0073 - val_acc: 0.2750\n",
      "Epoch 25/25\n",
      "45000/45000 [==============================] - 168s - loss: 3.0667 - acc: 0.2476 - val_loss: 3.0214 - val_acc: 0.2770\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 174s - loss: 3.0563 - acc: 0.2500 - val_loss: 3.0018 - val_acc: 0.2754\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 168s - loss: 3.0587 - acc: 0.2480 - val_loss: 2.9895 - val_acc: 0.2824\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 168s - loss: 3.0523 - acc: 0.2504 - val_loss: 2.9877 - val_acc: 0.2726\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 167s - loss: 3.0559 - acc: 0.2499 - val_loss: 2.9879 - val_acc: 0.2714\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 168s - loss: 3.0442 - acc: 0.2533 - val_loss: 2.9977 - val_acc: 0.2732\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 31/35\n",
      "45000/45000 [==============================] - 167s - loss: 3.0493 - acc: 0.2511 - val_loss: 2.9869 - val_acc: 0.2730\n",
      "Epoch 32/35\n",
      "45000/45000 [==============================] - 168s - loss: 3.0557 - acc: 0.2524 - val_loss: 2.9818 - val_acc: 0.2744\n",
      "Epoch 33/35\n",
      "45000/45000 [==============================] - 168s - loss: 3.0473 - acc: 0.2523 - val_loss: 2.9900 - val_acc: 0.2792\n",
      "Epoch 34/35\n",
      "45000/45000 [==============================] - 167s - loss: 3.0398 - acc: 0.2527 - val_loss: 2.9987 - val_acc: 0.2840\n",
      "Epoch 35/35\n",
      "45000/45000 [==============================] - 170s - loss: 3.0423 - acc: 0.2564 - val_loss: 2.9893 - val_acc: 0.2760\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 36/40\n",
      "45000/45000 [==============================] - 172s - loss: 3.0507 - acc: 0.2492 - val_loss: 2.9759 - val_acc: 0.2824\n",
      "Epoch 37/40\n",
      "45000/45000 [==============================] - 172s - loss: 3.0468 - acc: 0.2512 - val_loss: 2.9876 - val_acc: 0.2814\n",
      "Epoch 38/40\n",
      "45000/45000 [==============================] - 173s - loss: 3.0548 - acc: 0.2516 - val_loss: 3.0020 - val_acc: 0.2740\n",
      "Epoch 39/40\n",
      "45000/45000 [==============================] - 172s - loss: 3.0444 - acc: 0.2535 - val_loss: 3.0033 - val_acc: 0.2714\n",
      "Epoch 40/40\n",
      "45000/45000 [==============================] - 172s - loss: 3.0453 - acc: 0.2551 - val_loss: 3.0031 - val_acc: 0.2730\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 41/45\n",
      "45000/45000 [==============================] - 172s - loss: 3.0547 - acc: 0.2522 - val_loss: 2.9822 - val_acc: 0.2788\n",
      "Epoch 42/45\n",
      "45000/45000 [==============================] - 172s - loss: 3.0593 - acc: 0.2522 - val_loss: 2.9894 - val_acc: 0.2748\n",
      "Epoch 43/45\n",
      "45000/45000 [==============================] - 172s - loss: 3.0411 - acc: 0.2528 - val_loss: 2.9931 - val_acc: 0.2728\n",
      "Epoch 44/45\n",
      "45000/45000 [==============================] - 172s - loss: 3.0439 - acc: 0.2505 - val_loss: 2.9939 - val_acc: 0.2764\n",
      "Epoch 45/45\n",
      "45000/45000 [==============================] - 172s - loss: 3.0465 - acc: 0.2528 - val_loss: 2.9923 - val_acc: 0.2700\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 172s - loss: 3.0474 - acc: 0.2522 - val_loss: 2.9837 - val_acc: 0.2768\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 172s - loss: 3.0456 - acc: 0.2526 - val_loss: 2.9829 - val_acc: 0.2756\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 171s - loss: 3.0487 - acc: 0.2531 - val_loss: 2.9903 - val_acc: 0.2760\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 171s - loss: 3.0511 - acc: 0.2544 - val_loss: 2.9969 - val_acc: 0.2706\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 171s - loss: 3.0515 - acc: 0.2546 - val_loss: 2.9933 - val_acc: 0.2770\n"
     ]
    }
   ],
   "source": [
    "ck = np.zeros((20, len(x_train), len(C2K_dot[0])))\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(20):\n",
    "        ck[j,i] = C2K_dot[j]\n",
    "\n",
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, ck[0], ck[1], ck[2], ck[3], ck[4], ck[5], ck[6], ck[7], ck[8], ck[9], ck[10], ck[11], ck[12], ck[13], ck[14], ck[15], ck[16], ck[17], ck[18], ck[19]],\\\n",
    "              y_train_adj, initial_epoch=index, epochs=index+step, batch_size=256, validation_split=.1)\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, ck[0], ck[1], ck[2], ck[3], ck[4], ck[5], ck[6], ck[7], ck[8], ck[9], ck[10], ck[11], ck[12], ck[13], ck[14], ck[15], ck[16], ck[17], ck[18], ck[19]],\\\n",
    "          y_test_adj, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
