{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cifar100 Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "X = X[:20]\n",
    "y = y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: One Hot Encoding\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function extends a matrix to one-hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        y    Array of label values\n",
    "# \n",
    "#    Returns:\n",
    "#        y_new    One hot encoded array of labels\n",
    "################################################################################\n",
    "def one_hot(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    y_new = np.eye(n_values)[y[:,0]]\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=one_hot(y)\n",
    "y_test=one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize Images to be compatible with VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('ZCA'):\n",
    "    flatx = tf.cast(tf.reshape(X, (-1, np.prod(X.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n",
    "    sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64)\n",
    "    s, u, v = tf.svd(sigma,name=\"svd\")\n",
    "    net = tf.tensordot(tf.tensordot(u,tf.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n",
    "    net = tf.tensordot(flatx, net,1,name=\"whiten\")\n",
    "    net = tf.reshape(net,np.shape(x_batch), name=\"output\")\n",
    "with tf.Session() as sess:\n",
    "        zca[ind:end] = sess.run(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Preprocess Images\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function resizes 32x32x3 images to 128x128x3 by adding padding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            Array of 32x32x3 Images\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        A 224x224 set of images\n",
    "################################################################################\n",
    "def preproc_images(X, num_batch):\n",
    "    l = len(X)\n",
    "    d=int(l/num_batch)\n",
    "    X_new = np.zeros((l,224,224,3)).astype(np.int8)\n",
    "    ind = 0\n",
    "    for i in range (num_batch):\n",
    "        end = ind+d\n",
    "        if i==num_batch-1:\n",
    "            end = l;\n",
    "        x_batch = X[ind:end]\n",
    "        net = tf.image.resize_images(x_batch, size=(256,256))\n",
    "        net = tf.image.resize_image_with_crop_or_pad(net, 224, 224)\n",
    "        with tf.Session() as sess:\n",
    "                X_new[ind:end] = sess.run(net)\n",
    "        ind = ind + d\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: ZCA\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function applies ZCA Whitening to the image set\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            Array of MxNxC images\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def zca(X, num_batch, epsilon=1e-5):\n",
    "    l,M,N,C = np.shape(X)\n",
    "    d=int(l/num_batch)\n",
    "    zca = np.zeros((l,M,N,C))\n",
    "    ind = 0\n",
    "    for i in range (num_batch):\n",
    "        end = ind+d\n",
    "        if i==num_batch-1: end = l;\n",
    "        x_batch = X[ind:end]\n",
    "        \n",
    "        with tf.name_scope('ZCA'):\n",
    "            flatx = tf.cast(tf.reshape(x_batch, (-1, np.prod(x_batch.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n",
    "            sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64)\n",
    "            s, u, v = tf.svd(sigma,name=\"svd\")\n",
    "            net = tf.tensordot(tf.tensordot(u,tf.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n",
    "            net = tf.tensordot(flatx, net,1,name=\"whiten\")\n",
    "            net = tf.reshape(net,np.shape(x_batch), name=\"output\")\n",
    "        with tf.Session() as sess:\n",
    "                zca[ind:end] = sess.run(net)\n",
    "        ind = end\n",
    "    return zca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zca(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "X = preproc_images(X,4)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - Resizing: '+str(time2-time1));\n",
    "X = zca(X,4)\n",
    "time3 = time.time()\n",
    "print('Time Elapsed - ZCA Whitening: '+str(time3-time2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training set into Training and Validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import VGG16 Pretrained on Imagenet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation credit for VGG16 model to:\n",
    "\n",
    "Simonyan, Karen, and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” [1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition, 10 Apr. 2015, arxiv.org/abs/1409.1556."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv3D\n",
    "from keras.models import Model\n",
    "in_layer = Input(shape=(224, 224, 3), dtype='float32', name='main_input')\n",
    "model = VGG16(include_top=True, weights='imagenet', input_tensor=in_layer, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify Model for Cifar100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_coarse = Dense(fine_categories, activation='softmax')(model.layers[-2].output)\n",
    "model = Model(input=in_layer,output=out_coarse)\n",
    "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    zca_whitening=True,\n",
    "    zca_epsilon=1e-6,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "time2 = time.time()\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 300\n",
    "\n",
    "while index < stop:\n",
    "    model.fit(x_train, y_train, batch_size=32, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val))\n",
    "    index += step\n",
    "    model.save_weights('data/models/model_coarse_'+str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = 5\n",
    "model.load_weights('data/models/model_coarse_'+str(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainable_index = 17\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    if i<trainable_index:\n",
    "        model.layers[i].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_model():\n",
    "    out_fine = Dense(fine_categories, activation='softmax')(model.layers[-2].output)\n",
    "    model_fine = Model(input=in_layer,output=out_fine)\n",
    "    model_fine.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models = {'models' : [{} for i in range(coarse_categories)]}\n",
    "print(fine_models['models'])\n",
    "for i in range(coarse_categories):\n",
    "    model_i = fine_model()\n",
    "    fine_models['models'][i] = model_i\n",
    "print(fine_models['models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(np.array(y_train)[:,1] == 1)\n",
    "y_i = [y_train[j] for j in index]\n",
    "x_i = [x_train[j] for j in index]\n",
    "print(np.shape(y_i[0]))\n",
    "print(np.shape(x_i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(coarse_categories):\n",
    "    print(\"Training Fine Classifier: \", str(i))\n",
    "    \n",
    "    index= 0\n",
    "    step = 5\n",
    "    stop = 5\n",
    "\n",
    "    ind = np.where(np.array(y_train)[:,i] == 1)\n",
    "    y_i = [y_train[j] for j in ind]\n",
    "    x_i = [x_train[j] for j in ind]\n",
    "    \n",
    "    indv = np.where(np.array(y_val)[:,i] == 1)\n",
    "    y_iv = [y_val[j] for j in indv]\n",
    "    x_iv = [x_val[j] for j in indv]\n",
    "    \n",
    "    print(np.shape(x_iv))\n",
    "    while index < stop:\n",
    "        fine_models['models'][i].fit(x_i, y_i, batch_size=2, initial_epoch=index, epochs=index+step, validation_data=(x_iv, y_iv))\n",
    "        index += step\n",
    "        fine_models['models'][i].save_weights('data/models/model_fine_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
