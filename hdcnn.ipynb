{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import division, print_function, unicode_literals, absolute_import\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# To make make consistent across code blocks\n",
    "rnd.seed(42)\n",
    "\n",
    "# Ploting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Preprocessing functions\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Saving Parameters\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"data\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To Normalized\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function shifts image data to be centered around 0 and bounded\n",
    "#        by negative one and one.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array containing image data of range [0, 255]\n",
    "# \n",
    "#    Returns:\n",
    "#        Same array centered around 0 and bounded by [-1, 1]\n",
    "################################################################################\n",
    "def to_normalized(X):\n",
    "    X = (X-128)/128\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To One Hot\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function encodes label data with one hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Get Cifar Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function gets the Cifar dataset\n",
    "# \n",
    "#    Returns:\n",
    "#        Arrays containing training, testing, and validation data\n",
    "################################################################################\n",
    "def get_cifar_dataset():\n",
    "    # Import data set\n",
    "    from keras.datasets import cifar100\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # Adjust image data to center around 0 and be bound by [-1, 1]\n",
    "    x_train = to_normalized(x_train)\n",
    "    x_test = to_normalized(x_test)\n",
    "\n",
    "    # Encode labels as one hot\n",
    "    y_train = to_one_hot(y_train)\n",
    "    y_test  = to_one_hot(y_test)\n",
    "    \n",
    "    # Split training set into train and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=0)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Clip and Flip\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This clips images into the corners and center images then flips each\n",
    "#        such that the resulting concatenated set is 10x as large\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of image data\n",
    "#        Y    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def clip_n_flip(X, Y):\n",
    "    Y_conc = np.concatenate([Y, Y, Y, Y, Y, Y, Y, Y, Y, Y], axis=0)\n",
    "    X_a = X[:,:28,:28]\n",
    "    X_b = X[:,4:,:28]\n",
    "    X_c = X[:,:28,4:]\n",
    "    X_d = X[:,4:,4:]\n",
    "    X_e = X[:,2:30,2:30]\n",
    "    X_clip = np.concatenate([X_a,X_b,X_c,X_d,X_e], axis=0)\n",
    "    X_flip = np.concatenate([X_clip, np.fliplr(X_clip)], axis=0)\n",
    "    return X_flip, Y_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Read or Load Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function loads the dataset and saves a pickle file if one does not\n",
    "#        already exist.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def read_or_load_dataset(path):\n",
    "    try:\n",
    "        # Load data from a file if it exists\n",
    "        with open(path, \"rb\") as f:\n",
    "            x_train = pickle.load(f)\n",
    "            y_train = pickle.load(f)\n",
    "            x_val = pickle.load(f)\n",
    "            y_val = pickle.load(f)\n",
    "            x_test = pickle.load(f)\n",
    "            y_test = pickle.load(f)\n",
    "    \n",
    "    except StandardError:\n",
    "        # Get dataset\n",
    "        (x_train, y_train), (x_val, y_val), (x_test, y_test) = get_cifar_dataset\n",
    "        \n",
    "        # Clip and flip the images\n",
    "        x_train, y_train = clip_n_flip(x_train, y_train)\n",
    "\n",
    "        # Shuffle the images\n",
    "        X_out, Y_out = shuffle(x_train, y_train, random_state=1)\n",
    "\n",
    "        # Clip the validation and test sets to match training set\n",
    "        x_val = x_val[:,2:30,2:30]\n",
    "        x_test = x_test[:,2:30,2:30]\n",
    "        \n",
    "        # Save data to a file\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(x_train, f)\n",
    "            pickle.dump(y_train, f)\n",
    "            pickle.dump(x_val, f)\n",
    "            pickle.dump(y_val, f)\n",
    "            pickle.dump(x_test, f)\n",
    "            pickle.dump(y_test, f)\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"data/dataset.pickle\"\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = read_or_load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Independent Layers\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function creates the set of layers that the coarse and each fine\n",
    "#        classifier will have.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            The last tensor in the shared set of tensors\n",
    "#        set_index    The index of the classifier section \n",
    "# \n",
    "#    Returns:\n",
    "#        Last tensor prior to the softmax layer\n",
    "################################################################################\n",
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(128, (2, 2), strides=(1, 1), padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = Flatten()(net)\n",
    "\n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Dense(64, activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Concatenate, Dot, RepeatVector, Reshape, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "input_img = Input(shape=(28, 28, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(16, (4, 4), strides=(1, 1), padding='same', activation='elu', name='shared_layer_1')(input_img)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "\n",
    "net_shared = Conv2D(32, (4, 4), strides=(1, 1), padding='same', activation='elu', name='shared_layer_2')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "\n",
    "net_shared = Conv2D(64, (4, 4), strides=(1, 1), padding='same', activation='elu', name='shared_layer_3')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "net = indep_layers(net_shared, '0')\n",
    "output_c = Dense(fine_categories, activation='softmax')(net)\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base_model.load_weights('data/models/base_model_70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 1/300\n",
      "450000/450000 [==============================] - 50s - loss: 4.0407 - acc: 0.0848 - val_loss: 3.5379 - val_acc: 0.1718\n",
      "Epoch 2/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.6622 - acc: 0.1400 - val_loss: 3.3605 - val_acc: 0.1838\n",
      "Epoch 3/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.5276 - acc: 0.1609 - val_loss: 3.2135 - val_acc: 0.2078\n",
      "Epoch 4/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.4417 - acc: 0.1729 - val_loss: 3.1154 - val_acc: 0.2338\n",
      "Epoch 5/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.3741 - acc: 0.1845 - val_loss: 3.0520 - val_acc: 0.2316\n",
      "Epoch 6/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.3250 - acc: 0.1928 - val_loss: 3.0515 - val_acc: 0.2392\n",
      "Epoch 7/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.2880 - acc: 0.2002 - val_loss: 2.9826 - val_acc: 0.2520\n",
      "Epoch 8/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.2545 - acc: 0.2065 - val_loss: 2.9579 - val_acc: 0.2582\n",
      "Epoch 9/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.2247 - acc: 0.2104 - val_loss: 2.8945 - val_acc: 0.2760\n",
      "Epoch 10/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.2039 - acc: 0.2151 - val_loss: 2.8882 - val_acc: 0.2730\n",
      "Epoch 11/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.1789 - acc: 0.2204 - val_loss: 2.8576 - val_acc: 0.2798\n",
      "Epoch 12/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.1612 - acc: 0.2236 - val_loss: 2.8840 - val_acc: 0.2754\n",
      "Epoch 13/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.1458 - acc: 0.2260 - val_loss: 2.8418 - val_acc: 0.2800\n",
      "Epoch 14/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.1322 - acc: 0.2287 - val_loss: 2.8423 - val_acc: 0.2800\n",
      "Epoch 15/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.1191 - acc: 0.2312 - val_loss: 2.8378 - val_acc: 0.2864\n",
      "Epoch 16/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.1067 - acc: 0.2334 - val_loss: 2.8184 - val_acc: 0.2868\n",
      "Epoch 17/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0916 - acc: 0.2363 - val_loss: 2.8111 - val_acc: 0.2894\n",
      "Epoch 18/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0810 - acc: 0.2382 - val_loss: 2.8366 - val_acc: 0.2880\n",
      "Epoch 19/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0713 - acc: 0.2395 - val_loss: 2.7785 - val_acc: 0.2966\n",
      "Epoch 20/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0627 - acc: 0.2411 - val_loss: 2.7776 - val_acc: 0.2974\n",
      "Epoch 21/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.0551 - acc: 0.2429 - val_loss: 2.7494 - val_acc: 0.3018\n",
      "Epoch 22/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0478 - acc: 0.2440 - val_loss: 2.7525 - val_acc: 0.3090\n",
      "Epoch 23/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0392 - acc: 0.2455 - val_loss: 2.7437 - val_acc: 0.3066\n",
      "Epoch 24/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0325 - acc: 0.2475 - val_loss: 2.7425 - val_acc: 0.3030\n",
      "Epoch 25/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0255 - acc: 0.2480 - val_loss: 2.7489 - val_acc: 0.2984\n",
      "Epoch 26/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0245 - acc: 0.2486 - val_loss: 2.7711 - val_acc: 0.2968\n",
      "Epoch 27/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0154 - acc: 0.2511 - val_loss: 2.7210 - val_acc: 0.3130\n",
      "Epoch 28/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.0083 - acc: 0.2521 - val_loss: 2.7608 - val_acc: 0.2956\n",
      "Epoch 29/300\n",
      "450000/450000 [==============================] - 47s - loss: 3.0051 - acc: 0.2533 - val_loss: 2.7127 - val_acc: 0.3120\n",
      "Epoch 30/300\n",
      "450000/450000 [==============================] - 48s - loss: 3.0003 - acc: 0.2545 - val_loss: 2.7260 - val_acc: 0.3050\n",
      "Epoch 31/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.9940 - acc: 0.2557 - val_loss: 2.7221 - val_acc: 0.3092\n",
      "Epoch 32/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.9888 - acc: 0.2562 - val_loss: 2.7136 - val_acc: 0.3120\n",
      "Epoch 33/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9868 - acc: 0.2568 - val_loss: 2.7256 - val_acc: 0.3126\n",
      "Epoch 34/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9771 - acc: 0.2589 - val_loss: 2.7579 - val_acc: 0.3060\n",
      "Epoch 35/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9768 - acc: 0.2588 - val_loss: 2.7383 - val_acc: 0.3098\n",
      "Epoch 36/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9754 - acc: 0.2598 - val_loss: 2.6967 - val_acc: 0.3180\n",
      "Epoch 37/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9699 - acc: 0.2603 - val_loss: 2.6928 - val_acc: 0.3168\n",
      "Epoch 38/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9657 - acc: 0.2611 - val_loss: 2.7086 - val_acc: 0.3072\n",
      "Epoch 39/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9614 - acc: 0.2621 - val_loss: 2.7177 - val_acc: 0.3078\n",
      "Epoch 40/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9605 - acc: 0.2620 - val_loss: 2.6890 - val_acc: 0.3128\n",
      "Epoch 41/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9545 - acc: 0.2636 - val_loss: 2.6838 - val_acc: 0.3130\n",
      "Epoch 42/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9496 - acc: 0.2637 - val_loss: 2.7030 - val_acc: 0.3128\n",
      "Epoch 43/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9476 - acc: 0.2648 - val_loss: 2.6896 - val_acc: 0.3104\n",
      "Epoch 44/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9442 - acc: 0.2645 - val_loss: 2.6762 - val_acc: 0.3166\n",
      "Epoch 45/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9443 - acc: 0.2651 - val_loss: 2.6876 - val_acc: 0.3126\n",
      "Epoch 46/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9385 - acc: 0.2671 - val_loss: 2.6870 - val_acc: 0.3188\n",
      "Epoch 47/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9368 - acc: 0.2665 - val_loss: 2.6869 - val_acc: 0.3144\n",
      "Epoch 48/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9301 - acc: 0.2679 - val_loss: 2.6878 - val_acc: 0.3144\n",
      "Epoch 49/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9348 - acc: 0.2666 - val_loss: 2.7166 - val_acc: 0.3080\n",
      "Epoch 50/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9307 - acc: 0.2684 - val_loss: 2.6689 - val_acc: 0.3194\n",
      "Epoch 51/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9301 - acc: 0.2685 - val_loss: 2.6815 - val_acc: 0.3202\n",
      "Epoch 52/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9278 - acc: 0.2687 - val_loss: 2.7070 - val_acc: 0.3108\n",
      "Epoch 53/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9202 - acc: 0.2690 - val_loss: 2.6662 - val_acc: 0.3258\n",
      "Epoch 54/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9184 - acc: 0.2704 - val_loss: 2.6581 - val_acc: 0.3250\n",
      "Epoch 55/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9180 - acc: 0.2715 - val_loss: 2.6827 - val_acc: 0.3220\n",
      "Epoch 56/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.9194 - acc: 0.2703 - val_loss: 2.6669 - val_acc: 0.3242\n",
      "Epoch 57/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.9149 - acc: 0.2717 - val_loss: 2.6599 - val_acc: 0.3306\n",
      "Epoch 58/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9140 - acc: 0.2710 - val_loss: 2.6553 - val_acc: 0.3176\n",
      "Epoch 59/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9108 - acc: 0.2724 - val_loss: 2.6514 - val_acc: 0.3212\n",
      "Epoch 60/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9075 - acc: 0.2729 - val_loss: 2.6782 - val_acc: 0.3200\n",
      "Epoch 61/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9056 - acc: 0.2727 - val_loss: 2.6773 - val_acc: 0.3186\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 48s - loss: 2.9013 - acc: 0.2739 - val_loss: 2.6244 - val_acc: 0.3248\n",
      "Epoch 63/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9029 - acc: 0.2730 - val_loss: 2.6612 - val_acc: 0.3194\n",
      "Epoch 64/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8987 - acc: 0.2748 - val_loss: 2.6708 - val_acc: 0.3214\n",
      "Epoch 65/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8975 - acc: 0.2752 - val_loss: 2.6685 - val_acc: 0.3290\n",
      "Epoch 66/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.9034 - acc: 0.2743 - val_loss: 2.6329 - val_acc: 0.3264\n",
      "Epoch 67/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8961 - acc: 0.2757 - val_loss: 2.6506 - val_acc: 0.3298\n",
      "Epoch 68/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8933 - acc: 0.2753 - val_loss: 2.6284 - val_acc: 0.3230\n",
      "Epoch 69/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8939 - acc: 0.2751 - val_loss: 2.6350 - val_acc: 0.3232\n",
      "Epoch 70/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8924 - acc: 0.2765 - val_loss: 2.6198 - val_acc: 0.3312\n",
      "Epoch 71/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8878 - acc: 0.2769 - val_loss: 2.6449 - val_acc: 0.3232\n",
      "Epoch 72/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8865 - acc: 0.2778 - val_loss: 2.6555 - val_acc: 0.3308\n",
      "Epoch 73/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8883 - acc: 0.2775 - val_loss: 2.6534 - val_acc: 0.3256\n",
      "Epoch 74/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8856 - acc: 0.2767 - val_loss: 2.6431 - val_acc: 0.3240\n",
      "Epoch 75/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8835 - acc: 0.2769 - val_loss: 2.6307 - val_acc: 0.3240\n",
      "Epoch 76/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8796 - acc: 0.2789 - val_loss: 2.6303 - val_acc: 0.3294\n",
      "Epoch 77/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8815 - acc: 0.2786 - val_loss: 2.6329 - val_acc: 0.3274\n",
      "Epoch 78/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8784 - acc: 0.2788 - val_loss: 2.6273 - val_acc: 0.3300\n",
      "Epoch 79/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8748 - acc: 0.2796 - val_loss: 2.6158 - val_acc: 0.3384\n",
      "Epoch 80/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8776 - acc: 0.2791 - val_loss: 2.5969 - val_acc: 0.3414\n",
      "Epoch 81/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8753 - acc: 0.2793 - val_loss: 2.6217 - val_acc: 0.3328\n",
      "Epoch 82/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8752 - acc: 0.2793 - val_loss: 2.6459 - val_acc: 0.3264\n",
      "Epoch 83/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8711 - acc: 0.2804 - val_loss: 2.6578 - val_acc: 0.3242\n",
      "Epoch 84/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8704 - acc: 0.2799 - val_loss: 2.6370 - val_acc: 0.3228\n",
      "Epoch 85/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8698 - acc: 0.2816 - val_loss: 2.6440 - val_acc: 0.3254\n",
      "Epoch 86/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8751 - acc: 0.2800 - val_loss: 2.6409 - val_acc: 0.3238\n",
      "Epoch 87/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8683 - acc: 0.2814 - val_loss: 2.6749 - val_acc: 0.3260\n",
      "Epoch 88/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8689 - acc: 0.2815 - val_loss: 2.6083 - val_acc: 0.3336\n",
      "Epoch 89/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8730 - acc: 0.2800 - val_loss: 2.6284 - val_acc: 0.3252\n",
      "Epoch 90/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8697 - acc: 0.2821 - val_loss: 2.6031 - val_acc: 0.3366\n",
      "Epoch 91/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8670 - acc: 0.2814 - val_loss: 2.6194 - val_acc: 0.3314\n",
      "Epoch 92/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8658 - acc: 0.2811 - val_loss: 2.6321 - val_acc: 0.3236\n",
      "Epoch 93/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8621 - acc: 0.2828 - val_loss: 2.6081 - val_acc: 0.3358\n",
      "Epoch 94/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8586 - acc: 0.2833 - val_loss: 2.6624 - val_acc: 0.3180\n",
      "Epoch 95/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8611 - acc: 0.2833 - val_loss: 2.6853 - val_acc: 0.3192\n",
      "Epoch 96/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8599 - acc: 0.2830 - val_loss: 2.6487 - val_acc: 0.3300\n",
      "Epoch 97/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8559 - acc: 0.2836 - val_loss: 2.6265 - val_acc: 0.3292\n",
      "Epoch 98/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8571 - acc: 0.2839 - val_loss: 2.6366 - val_acc: 0.3334\n",
      "Epoch 99/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8566 - acc: 0.2846 - val_loss: 2.6273 - val_acc: 0.3314\n",
      "Epoch 100/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8547 - acc: 0.2841 - val_loss: 2.5939 - val_acc: 0.3360\n",
      "Epoch 101/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8566 - acc: 0.2839 - val_loss: 2.5944 - val_acc: 0.3390\n",
      "Epoch 102/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8563 - acc: 0.2837 - val_loss: 2.6185 - val_acc: 0.3272\n",
      "Epoch 103/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8576 - acc: 0.2847 - val_loss: 2.6642 - val_acc: 0.3198\n",
      "Epoch 104/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8557 - acc: 0.2838 - val_loss: 2.6307 - val_acc: 0.3332\n",
      "Epoch 105/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8542 - acc: 0.2841 - val_loss: 2.6126 - val_acc: 0.3362\n",
      "Epoch 106/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8501 - acc: 0.2847 - val_loss: 2.6213 - val_acc: 0.3298\n",
      "Epoch 107/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8486 - acc: 0.2846 - val_loss: 2.6153 - val_acc: 0.3302\n",
      "Epoch 108/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8471 - acc: 0.2861 - val_loss: 2.5938 - val_acc: 0.3354\n",
      "Epoch 109/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8504 - acc: 0.2849 - val_loss: 2.6053 - val_acc: 0.3368\n",
      "Epoch 110/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8482 - acc: 0.2855 - val_loss: 2.6176 - val_acc: 0.3258\n",
      "Epoch 111/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8467 - acc: 0.2854 - val_loss: 2.6230 - val_acc: 0.3278\n",
      "Epoch 112/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8484 - acc: 0.2858 - val_loss: 2.5921 - val_acc: 0.3290\n",
      "Epoch 113/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8480 - acc: 0.2857 - val_loss: 2.6371 - val_acc: 0.3190\n",
      "Epoch 114/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8440 - acc: 0.2863 - val_loss: 2.6631 - val_acc: 0.3254\n",
      "Epoch 115/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8416 - acc: 0.2871 - val_loss: 2.6652 - val_acc: 0.3170\n",
      "Epoch 116/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8449 - acc: 0.2865 - val_loss: 2.6121 - val_acc: 0.3314\n",
      "Epoch 117/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8425 - acc: 0.2871 - val_loss: 2.6127 - val_acc: 0.3316\n",
      "Epoch 118/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8392 - acc: 0.2869 - val_loss: 2.6273 - val_acc: 0.3224\n",
      "Epoch 119/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8397 - acc: 0.2865 - val_loss: 2.5915 - val_acc: 0.3368\n",
      "Epoch 120/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8421 - acc: 0.2858 - val_loss: 2.6098 - val_acc: 0.3352\n",
      "Epoch 121/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8374 - acc: 0.2867 - val_loss: 2.6491 - val_acc: 0.3176\n",
      "Epoch 122/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8419 - acc: 0.2874 - val_loss: 2.5915 - val_acc: 0.3326\n",
      "Epoch 123/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 47s - loss: 2.8379 - acc: 0.2876 - val_loss: 2.6052 - val_acc: 0.3340\n",
      "Epoch 124/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8377 - acc: 0.2868 - val_loss: 2.6105 - val_acc: 0.3312\n",
      "Epoch 125/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8382 - acc: 0.2870 - val_loss: 2.5899 - val_acc: 0.3364\n",
      "Epoch 126/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8357 - acc: 0.2887 - val_loss: 2.6254 - val_acc: 0.3264\n",
      "Epoch 127/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8380 - acc: 0.2880 - val_loss: 2.6077 - val_acc: 0.3360\n",
      "Epoch 128/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8357 - acc: 0.2875 - val_loss: 2.6174 - val_acc: 0.3308\n",
      "Epoch 129/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8365 - acc: 0.2878 - val_loss: 2.6118 - val_acc: 0.3334\n",
      "Epoch 130/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8348 - acc: 0.2881 - val_loss: 2.6596 - val_acc: 0.3192\n",
      "Epoch 131/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8356 - acc: 0.2888 - val_loss: 2.6161 - val_acc: 0.3288\n",
      "Epoch 132/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8334 - acc: 0.2879 - val_loss: 2.6186 - val_acc: 0.3284\n",
      "Epoch 133/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8323 - acc: 0.2886 - val_loss: 2.5856 - val_acc: 0.3290\n",
      "Epoch 134/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8329 - acc: 0.2882 - val_loss: 2.5909 - val_acc: 0.3306\n",
      "Epoch 135/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8330 - acc: 0.2889 - val_loss: 2.6100 - val_acc: 0.3290\n",
      "Epoch 136/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8366 - acc: 0.2882 - val_loss: 2.5900 - val_acc: 0.3368\n",
      "Epoch 137/300\n",
      "450000/450000 [==============================] - 47s - loss: 2.8304 - acc: 0.2900 - val_loss: 2.6242 - val_acc: 0.3364\n",
      "Epoch 138/300\n",
      "450000/450000 [==============================] - 48s - loss: 2.8265 - acc: 0.2901 - val_loss: 2.6312 - val_acc: 0.3260\n",
      "Epoch 139/300\n",
      " 98560/450000 [=====>........................] - ETA: 37s - loss: 2.8222 - acc: 0.2878"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "base_model.fit(x_train, y_train, initial_epoch=0, epochs=num_epochs, batch_size=256, validation_data=(x_val, y_val))\n",
    "base_model.save_weights('data/models/base_model_'+str(num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "##### THRESHOLDING LAYER IS NOT FUNCTIONAL #####\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    This layer takes in a set of coarse probabilities and an\n",
    "    index pointing to the relevant coarse probability\n",
    "    \n",
    "    It then thresholds that probability following:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    \n",
    "    Finally it multiplies that thresholded value across the \n",
    "    full input layer.\n",
    "    \n",
    "    # Input shape\n",
    "        Arbitrary 4-D Tensor\n",
    "    # Output\n",
    "        An tensor with the same shape as the input but with values\n",
    "        that are either all 1s or all 0s\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, index, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "        self.index = index\n",
    "        self.prob = prob\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Get Shape of Input\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        # Pull out coarse probability as specified by self.prob\n",
    "        X = self.prob[:,self.index:self.index+1]\n",
    "        # Threshold probability\n",
    "        X = K.cast(K.greater(X, self.theta), K.floatx())\n",
    "        # Repeat and Reshape thresholded probability to match input\n",
    "        X = K.tile(X, [1, shape[1]*shape[2]*shape[3]])\n",
    "        X = K.reshape(X, [-1,shape[1],shape[2],shape[3]])\n",
    "        # Multiply input with thresholded probability\n",
    "        X = Multiply()([X,inputs])\n",
    "        return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Calculate Probabalistic Averaging Layer\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function calculates the coarse classifier probabilities and\n",
    "#        extends that to a probabalistic averaging over fine classifications\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X      The output of the coarse classifier\n",
    "# \n",
    "#    Returns:\n",
    "#        Coarse category predictions and probabalistic averaging layer\n",
    "################################################################################\n",
    "def calc_prob_ave(X):\n",
    "    # Coarse to Fine mapping\n",
    "    ck = Input(shape=(20, 100), dtype='float32', name='C2K')\n",
    "    \n",
    "    # Extend input dimension from (n,100) to (n,1,100)\n",
    "    X = RepeatVector(1)(X)\n",
    "    \n",
    "    # Dot input, (n,1,100) and coarse to fine mapping (n,20,100) on axis 2\n",
    "    X = Dot(axes=2)([ck,X])\n",
    "    # Flatten result to (n,20)\n",
    "    Y = Reshape([20])(X)\n",
    "    \n",
    "    # Dot the vectors again to get probabalistic averaging layer (n,20,100)*(n,20,1) = (n,1,100)\n",
    "    X = Dot(axes=1)([ck,X])\n",
    "    # Flatten the result to (n,100)\n",
    "    X = Reshape([100])(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
