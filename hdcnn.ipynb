{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cifar100 Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training set into Training and Validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize Images to be compatible with VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Resize Images\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function resizes 32x32 images to 128x128 and then adds padding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            Array of label data\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        A 224x224 set of images\n",
    "################################################################################\n",
    "def resize_images(X, num_batch):\n",
    "    l = len(X)\n",
    "    d=int(l/num_batch)\n",
    "    X_new = np.zeros((l,224,224,3))\n",
    "    ind = 0\n",
    "    for i in range (num_batch):\n",
    "        end = ind+d\n",
    "        if i==num_batch-1:\n",
    "            end = l;\n",
    "        x_batch = X[ind:end]\n",
    "        net = tf.image.resize_images(x_batch, size=(128,128))\n",
    "        net = tf.image.resize_image_with_crop_or_pad(net, 224, 224)\n",
    "        with tf.Session() as sess:\n",
    "                X_new[ind:end] = sess.run(net)\n",
    "        ind = ind + d\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(45000, 224, 224, 3)\n",
      "Time Elapsed While Resizing:828.6972296237946\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "print(np.shape(x_train))\n",
    "x_val = resize_images(x_val,50)\n",
    "x_train = resize_images(x_train,450)\n",
    "print(np.shape(x_train))\n",
    "time2 = time.time()\n",
    "print('Time Elapsed While Resizing:'+str(time2-time1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To One Hot\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function encodes label data with one hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Independent Layers\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function creates the set of layers that the coarse and each fine\n",
    "#        classifier will have.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            The last tensor in the shared set of tensors\n",
    "#        set_index    The index of the classifier section \n",
    "# \n",
    "#    Returns:\n",
    "#        Last tensor prior to the softmax layer\n",
    "################################################################################\n",
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(128, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    \n",
    "    net = Conv2D(256, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(4))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(5))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(6))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Flatten()(net)\n",
    "    \n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(7))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Concatenate, Dot, RepeatVector, Reshape, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "input_img = Input(shape=(28, 28, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(64, 5, strides=1, padding='same', activation='elu', name='shared_layer_1')(input_img)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_2')(net_shared)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_3')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "net = indep_layers(net_shared, '0')\n",
    "output_c = Dense(fine_categories, activation='softmax')(net)\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.load_weights('data/models/base_model_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 5000 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cfb2843719b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/models/base_model_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         callbacks.set_params({\n\u001b[1;32m   1042\u001b[0m             \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    716\u001b[0m                           if layer.name in embeddings_layer_names}\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0membeddings_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "index= 150\n",
    "step = 5\n",
    "stop = 300\n",
    "\n",
    "while index < stop:\n",
    "    base_model.fit(x_train, y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "                   validation_data=(x_val, y_val))\n",
    "    index += step\n",
    "    base_model.save_weights('data/models/base_model_'+str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "##### THRESHOLDING LAYER IS NOT FUNCTIONAL #####\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    This layer takes in a set of coarse probabilities and an\n",
    "    index pointing to the relevant coarse probability\n",
    "    \n",
    "    It then thresholds that probability following:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    \n",
    "    Finally it multiplies that thresholded value across the \n",
    "    full input layer.\n",
    "    \n",
    "    # Input shape\n",
    "        Arbitrary 4-D Tensor\n",
    "    # Output\n",
    "        An tensor with the same shape as the input but with values\n",
    "        that are either all 1s or all 0s\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, index, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "        self.index = index\n",
    "        self.prob = prob\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Get Shape of Input\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        # Pull out coarse probability as specified by self.prob\n",
    "        X = self.prob[:,self.index:self.index+1]\n",
    "        # Threshold probability\n",
    "        X = K.cast(K.greater(X, self.theta), K.floatx())\n",
    "        # Repeat and Reshape thresholded probability to match input\n",
    "        X = K.tile(X, [1, shape[1]*shape[2]*shape[3]])\n",
    "        X = K.reshape(X, [-1,shape[1],shape[2],shape[3]])\n",
    "        # Multiply input with thresholded probability\n",
    "        X = Multiply()([X,inputs])\n",
    "        return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Calculate Probabalistic Averaging Layer\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function calculates the coarse classifier probabilities and\n",
    "#        extends that to a probabalistic averaging over fine classifications\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X      The output of the coarse classifier\n",
    "# \n",
    "#    Returns:\n",
    "#        Coarse category predictions and probabalistic averaging layer\n",
    "################################################################################\n",
    "def calc_prob_ave(X):\n",
    "    # Coarse to Fine mapping\n",
    "    ck = Input(shape=(20, 100), dtype='float32', name='C2K')\n",
    "    \n",
    "    # Extend input dimension from (n,100) to (n,1,100)\n",
    "    X = RepeatVector(1)(X)\n",
    "    \n",
    "    # Dot input, (n,1,100) and coarse to fine mapping (n,20,100) on axis 2\n",
    "    X = Dot(axes=2)([ck,X])\n",
    "    # Flatten result to (n,20)\n",
    "    Y = Reshape([20])(X)\n",
    "    \n",
    "    # Dot the vectors again to get probabalistic averaging layer (n,20,100)*(n,20,1) = (n,1,100)\n",
    "    X = Dot(axes=1)([ck,X])\n",
    "    # Flatten the result to (n,100)\n",
    "    X = Reshape([100])(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
