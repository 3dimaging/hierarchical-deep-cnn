{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import division, print_function, unicode_literals, absolute_import\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# To make make consistent across code blocks\n",
    "rnd.seed(42)\n",
    "\n",
    "# Ploting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Preprocessing functions\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Saving Parameters\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"data\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To Normalized\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function shifts image data to be centered around 0 and bounded\n",
    "#        by negative one and one.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array containing image data of range [0, 255]\n",
    "# \n",
    "#    Returns:\n",
    "#        Same array centered around 0 and bounded by [-1, 1]\n",
    "################################################################################\n",
    "def to_normalized(X):\n",
    "    X = (X-128)/128\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To One Hot\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function encodes label data with one hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Get Cifar Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function gets the Cifar dataset\n",
    "# \n",
    "#    Returns:\n",
    "#        Arrays containing training, testing, and validation data\n",
    "################################################################################\n",
    "def get_cifar_dataset():\n",
    "    # Import data set\n",
    "    from keras.datasets import cifar100\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # Adjust image data to center around 0 and be bound by [-1, 1]\n",
    "    x_train = to_normalized(x_train)\n",
    "    x_test = to_normalized(x_test)\n",
    "\n",
    "    # Encode labels as one hot\n",
    "    y_train = to_one_hot(y_train)\n",
    "    y_test  = to_one_hot(y_test)\n",
    "    \n",
    "    # Split training set into train and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=0)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Clip and Flip\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This clips images into the corners and center images then flips each\n",
    "#        such that the resulting concatenated set is 10x as large\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of image data\n",
    "#        Y    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def clip_n_flip(X, Y):\n",
    "    Y_conc = np.concatenate([Y, Y, Y, Y, Y, Y, Y, Y, Y, Y], axis=0)\n",
    "    X_a = X[:,:28,:28]\n",
    "    X_b = X[:,4:,:28]\n",
    "    X_c = X[:,:28,4:]\n",
    "    X_d = X[:,4:,4:]\n",
    "    X_e = X[:,2:30,2:30]\n",
    "    X_clip = np.concatenate([X_a,X_b,X_c,X_d,X_e], axis=0)\n",
    "    X_flip = np.concatenate([X_clip, np.fliplr(X_clip)], axis=0)\n",
    "    return X_flip, Y_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Read or Load Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function loads the dataset and saves a pickle file if one does not\n",
    "#        already exist.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def read_or_load_dataset(path):\n",
    "    try:\n",
    "        # Load data from a file if it exists\n",
    "        with open(path, \"rb\") as f:\n",
    "            x_train = pickle.load(f)\n",
    "            y_train = pickle.load(f)\n",
    "            x_val = pickle.load(f)\n",
    "            y_val = pickle.load(f)\n",
    "            x_test = pickle.load(f)\n",
    "            y_test = pickle.load(f)\n",
    "    \n",
    "    except StandardError:\n",
    "        # Get dataset\n",
    "        (x_train, y_train), (x_val, y_val), (x_test, y_test) = get_cifar_dataset\n",
    "        \n",
    "        # Clip and flip the images\n",
    "        x_train, y_train = clip_n_flip(x_train, y_train)\n",
    "\n",
    "        # Shuffle the images\n",
    "        X_out, Y_out = shuffle(x_train, y_train, random_state=1)\n",
    "\n",
    "        # Clip the validation and test sets to match training set\n",
    "        x_val = x_val[:,2:30,2:30]\n",
    "        x_test = x_test[:,2:30,2:30]\n",
    "        \n",
    "        # Save data to a file\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(x_train, f)\n",
    "            pickle.dump(y_train, f)\n",
    "            pickle.dump(x_val, f)\n",
    "            pickle.dump(y_val, f)\n",
    "            pickle.dump(x_test, f)\n",
    "            pickle.dump(y_test, f)\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"data/dataset.pickle\"\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = read_or_load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Independent Layers\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function creates the set of layers that the coarse and each fine\n",
    "#        classifier will have.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            The last tensor in the shared set of tensors\n",
    "#        set_index    The index of the classifier section \n",
    "# \n",
    "#    Returns:\n",
    "#        Last tensor prior to the softmax layer\n",
    "################################################################################\n",
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(128, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    \n",
    "    net = Conv2D(256, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(4))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(5))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(6))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Flatten()(net)\n",
    "    \n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(7))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Concatenate, Dot, RepeatVector, Reshape, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "input_img = Input(shape=(28, 28, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(64, 5, strides=1, padding='same', activation='elu', name='shared_layer_1')(input_img)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_2')(net_shared)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_3')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "net = indep_layers(net_shared, '0')\n",
    "output_c = Dense(fine_categories, activation='softmax')(net)\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base_model.load_weights('data/models/base_model_70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 1/300\n",
      "133632/450000 [=======>......................] - ETA: 141s - loss: 4.2694 - acc: 0.0592"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "base_model.fit(x_train, y_train, initial_epoch=0, epochs=num_epochs, batch_size=256, validation_data=(x_val, y_val))\n",
    "base_model.save_weights('data/models/base_model_'+str(num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "##### THRESHOLDING LAYER IS NOT FUNCTIONAL #####\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    This layer takes in a set of coarse probabilities and an\n",
    "    index pointing to the relevant coarse probability\n",
    "    \n",
    "    It then thresholds that probability following:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    \n",
    "    Finally it multiplies that thresholded value across the \n",
    "    full input layer.\n",
    "    \n",
    "    # Input shape\n",
    "        Arbitrary 4-D Tensor\n",
    "    # Output\n",
    "        An tensor with the same shape as the input but with values\n",
    "        that are either all 1s or all 0s\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, index, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "        self.index = index\n",
    "        self.prob = prob\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Get Shape of Input\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        # Pull out coarse probability as specified by self.prob\n",
    "        X = self.prob[:,self.index:self.index+1]\n",
    "        # Threshold probability\n",
    "        X = K.cast(K.greater(X, self.theta), K.floatx())\n",
    "        # Repeat and Reshape thresholded probability to match input\n",
    "        X = K.tile(X, [1, shape[1]*shape[2]*shape[3]])\n",
    "        X = K.reshape(X, [-1,shape[1],shape[2],shape[3]])\n",
    "        # Multiply input with thresholded probability\n",
    "        X = Multiply()([X,inputs])\n",
    "        return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Calculate Probabalistic Averaging Layer\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function calculates the coarse classifier probabilities and\n",
    "#        extends that to a probabalistic averaging over fine classifications\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X      The output of the coarse classifier\n",
    "# \n",
    "#    Returns:\n",
    "#        Coarse category predictions and probabalistic averaging layer\n",
    "################################################################################\n",
    "def calc_prob_ave(X):\n",
    "    # Coarse to Fine mapping\n",
    "    ck = Input(shape=(20, 100), dtype='float32', name='C2K')\n",
    "    \n",
    "    # Extend input dimension from (n,100) to (n,1,100)\n",
    "    X = RepeatVector(1)(X)\n",
    "    \n",
    "    # Dot input, (n,1,100) and coarse to fine mapping (n,20,100) on axis 2\n",
    "    X = Dot(axes=2)([ck,X])\n",
    "    # Flatten result to (n,20)\n",
    "    Y = Reshape([20])(X)\n",
    "    \n",
    "    # Dot the vectors again to get probabalistic averaging layer (n,20,100)*(n,20,1) = (n,1,100)\n",
    "    X = Dot(axes=1)([ck,X])\n",
    "    # Flatten the result to (n,100)\n",
    "    X = Reshape([100])(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
