{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import division, print_function, unicode_literals, absolute_import\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# To make make consistent across code blocks\n",
    "rnd.seed(42)\n",
    "\n",
    "# Ploting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Preprocessing functions\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Saving Parameters\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"data\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kr.callbacks import TensorBoard\n",
    "tbCallback = TensorBoard(log_dir='./data/logs', histogram_freq=1, batch_size=32, write_graph=True, embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To Normalized\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function shifts image data to be centered around 0 and bounded\n",
    "#        by negative one and one.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array containing image data of range [0, 255]\n",
    "# \n",
    "#    Returns:\n",
    "#        Same array centered around 0 and bounded by [-1, 1]\n",
    "################################################################################\n",
    "def to_normalized(X):\n",
    "    X = (X-128)/128\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: To One Hot\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function encodes label data with one hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def to_one_hot(X):\n",
    "    n_values = np.max(X) + 1\n",
    "    y = np.eye(n_values)[X[:,0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Get Cifar Dataset\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function gets the Cifar dataset\n",
    "# \n",
    "#    Returns:\n",
    "#        Arrays containing training, testing, and validation data\n",
    "################################################################################\n",
    "def get_cifar_dataset():\n",
    "    # Import data set\n",
    "    from keras.datasets import cifar100\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # Adjust image data to center around 0 and be bound by [-1, 1]\n",
    "    x_train = to_normalized(x_train)\n",
    "    x_test = to_normalized(x_test)\n",
    "\n",
    "    # Encode labels as one hot\n",
    "    y_train = to_one_hot(y_train)\n",
    "    y_test  = to_one_hot(y_test)\n",
    "    \n",
    "    # Split training set into train and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=0)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Clip and Flip\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This clips images into the corners and center images then flips each\n",
    "#        such that the resulting concatenated set is 10x as large\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X    Array of image data\n",
    "#        Y    Array of label data\n",
    "# \n",
    "#    Returns:\n",
    "#        One hot encoded array\n",
    "################################################################################\n",
    "def clip_n_flip(X, Y):\n",
    "    Y_conc = np.concatenate([Y, Y, Y, Y, Y, Y, Y, Y, Y, Y], axis=0)\n",
    "    X_a = X[:,:28,:28]\n",
    "    X_b = X[:,4:,:28]\n",
    "    X_c = X[:,:28,4:]\n",
    "    X_d = X[:,4:,4:]\n",
    "    X_e = X[:,2:30,2:30]\n",
    "    X_clip = np.concatenate([X_a,X_b,X_c,X_d,X_e], axis=0)\n",
    "    X_flip = np.concatenate([X_clip, np.fliplr(X_clip)], axis=0)\n",
    "    return X_flip, Y_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = get_cifar_dataset()\n",
    "\n",
    "# Clip and flip the images\n",
    "x_train, y_train = clip_n_flip(x_train, y_train)\n",
    "\n",
    "# Shuffle the images\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=1)\n",
    "\n",
    "# Clip the validation and test sets to match training set\n",
    "x_val = x_val[:,2:30,2:30]\n",
    "x_test = x_test[:,2:30,2:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100\n",
    "\n",
    "# The threshold percentage in thresholding layer\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Independent Layers\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function creates the set of layers that the coarse and each fine\n",
    "#        classifier will have.\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X            The last tensor in the shared set of tensors\n",
    "#        set_index    The index of the classifier section \n",
    "# \n",
    "#    Returns:\n",
    "#        Last tensor prior to the softmax layer\n",
    "################################################################################\n",
    "def indep_layers(X, set_index):\n",
    "    s = '__indep_layer_'\n",
    "    net = Conv2D(128, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(1))(X)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(2))(net)\n",
    "    net = Conv2D(128, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(3))(net)\n",
    "    net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    \n",
    "    net = Conv2D(256, 3, strides=1, padding='same', activation='elu', name=set_index+s+str(4))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(5))(net)\n",
    "    net = Conv2D(256, 1, strides=1, padding='same', activation='elu', name=set_index+s+str(6))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "    net = Flatten()(net)\n",
    "    \n",
    "    net = Dense(128, activation='elu', name=set_index+s+str(7))(net)\n",
    "    net = Dropout(0.50)(net)\n",
    "\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, Multiply, Add, Concatenate, Dot, RepeatVector, Reshape, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "input_img = Input(shape=(28, 28, 3), dtype='float32', name='main_input')\n",
    "\n",
    "## SHARED LAYERS\n",
    "net_shared = Conv2D(64, 5, strides=1, padding='same', activation='elu', name='shared_layer_1')(input_img)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_2')(net_shared)\n",
    "net_shared = Conv2D(64, 1, strides=1, padding='same', activation='elu', name='shared_layer_3')(net_shared)\n",
    "net_shared = MaxPooling2D((2, 2), padding='valid')(net_shared)\n",
    "net_shared = Dropout(0.50)(net_shared)\n",
    "\n",
    "# COARSE CLASSIFIER\n",
    "net = indep_layers(net_shared, '0')\n",
    "output_c = Dense(fine_categories, activation='softmax')(net)\n",
    "\n",
    "base_model = Model(inputs=input_img, outputs=output_c)\n",
    "\n",
    "base_model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.load_weights('data/models/base_model_25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 26/30\n",
      "450000/450000 [==============================] - 203s - loss: 2.7718 - acc: 0.2933 - val_loss: 2.6299 - val_acc: 0.3480\n",
      "Epoch 27/30\n",
      "450000/450000 [==============================] - 205s - loss: 2.6775 - acc: 0.3115 - val_loss: 2.5717 - val_acc: 0.3486\n",
      "Epoch 28/30\n",
      "450000/450000 [==============================] - 205s - loss: 2.6678 - acc: 0.3131 - val_loss: 2.5059 - val_acc: 0.3546\n",
      "Epoch 29/30\n",
      "450000/450000 [==============================] - 205s - loss: 2.6608 - acc: 0.3152 - val_loss: 2.5664 - val_acc: 0.3464\n",
      "Epoch 30/30\n",
      "450000/450000 [==============================] - 205s - loss: 2.6539 - acc: 0.3163 - val_loss: 2.4381 - val_acc: 0.3640\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 31/35\n",
      "450000/450000 [==============================] - 205s - loss: 2.6445 - acc: 0.3170 - val_loss: 2.6515 - val_acc: 0.3336\n",
      "Epoch 32/35\n",
      "450000/450000 [==============================] - 205s - loss: 2.6383 - acc: 0.3195 - val_loss: 2.5763 - val_acc: 0.3456\n",
      "Epoch 33/35\n",
      "450000/450000 [==============================] - 205s - loss: 2.6334 - acc: 0.3209 - val_loss: 2.4757 - val_acc: 0.3640\n",
      "Epoch 34/35\n",
      "450000/450000 [==============================] - 205s - loss: 2.6286 - acc: 0.3208 - val_loss: 2.6561 - val_acc: 0.3254\n",
      "Epoch 35/35\n",
      "450000/450000 [==============================] - 205s - loss: 2.6196 - acc: 0.3231 - val_loss: 2.4607 - val_acc: 0.3638\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 36/40\n",
      "450000/450000 [==============================] - 205s - loss: 2.6170 - acc: 0.3239 - val_loss: 2.4944 - val_acc: 0.3594\n",
      "Epoch 37/40\n",
      "450000/450000 [==============================] - 205s - loss: 2.6083 - acc: 0.3243 - val_loss: 2.4320 - val_acc: 0.3692\n",
      "Epoch 38/40\n",
      "450000/450000 [==============================] - 205s - loss: 2.6042 - acc: 0.3258 - val_loss: 2.5164 - val_acc: 0.3570\n",
      "Epoch 39/40\n",
      "450000/450000 [==============================] - 205s - loss: 2.5999 - acc: 0.3267 - val_loss: 2.5032 - val_acc: 0.3628\n",
      "Epoch 40/40\n",
      "450000/450000 [==============================] - 205s - loss: 2.5929 - acc: 0.3283 - val_loss: 2.4213 - val_acc: 0.3758\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 41/45\n",
      "450000/450000 [==============================] - 205s - loss: 2.5874 - acc: 0.3286 - val_loss: 2.5237 - val_acc: 0.3582\n",
      "Epoch 42/45\n",
      "450000/450000 [==============================] - 204s - loss: 2.5852 - acc: 0.3291 - val_loss: 2.4466 - val_acc: 0.3776\n",
      "Epoch 43/45\n",
      "450000/450000 [==============================] - 204s - loss: 2.5817 - acc: 0.3306 - val_loss: 2.4348 - val_acc: 0.3706\n",
      "Epoch 44/45\n",
      "450000/450000 [==============================] - 204s - loss: 2.5751 - acc: 0.3315 - val_loss: 2.5112 - val_acc: 0.3652\n",
      "Epoch 45/45\n",
      "450000/450000 [==============================] - 205s - loss: 2.5705 - acc: 0.3322 - val_loss: 2.3138 - val_acc: 0.3978\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 46/50\n",
      "450000/450000 [==============================] - 204s - loss: 2.5592 - acc: 0.3350 - val_loss: 2.3502 - val_acc: 0.3880\n",
      "Epoch 47/50\n",
      "450000/450000 [==============================] - 204s - loss: 2.5609 - acc: 0.3348 - val_loss: 2.4832 - val_acc: 0.3624\n",
      "Epoch 48/50\n",
      "450000/450000 [==============================] - 204s - loss: 2.5548 - acc: 0.3352 - val_loss: 2.3868 - val_acc: 0.3924\n",
      "Epoch 49/50\n",
      "450000/450000 [==============================] - 204s - loss: 2.5505 - acc: 0.3360 - val_loss: 2.4918 - val_acc: 0.3666\n",
      "Epoch 50/50\n",
      "450000/450000 [==============================] - 204s - loss: 2.5476 - acc: 0.3374 - val_loss: 2.5071 - val_acc: 0.3658\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 51/55\n",
      "450000/450000 [==============================] - 204s - loss: 2.5445 - acc: 0.3382 - val_loss: 2.4327 - val_acc: 0.3764\n",
      "Epoch 52/55\n",
      "450000/450000 [==============================] - 204s - loss: 2.5428 - acc: 0.3378 - val_loss: 2.4333 - val_acc: 0.3710\n",
      "Epoch 53/55\n",
      "450000/450000 [==============================] - 204s - loss: 2.5398 - acc: 0.3386 - val_loss: 2.3404 - val_acc: 0.3880\n",
      "Epoch 54/55\n",
      "450000/450000 [==============================] - 204s - loss: 2.5331 - acc: 0.3400 - val_loss: 2.3215 - val_acc: 0.3984\n",
      "Epoch 55/55\n",
      "450000/450000 [==============================] - 204s - loss: 2.5268 - acc: 0.3416 - val_loss: 2.4004 - val_acc: 0.3910\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 56/60\n",
      "450000/450000 [==============================] - 204s - loss: 2.5261 - acc: 0.3418 - val_loss: 2.3247 - val_acc: 0.3960\n",
      "Epoch 57/60\n",
      "450000/450000 [==============================] - 204s - loss: 2.5208 - acc: 0.3428 - val_loss: 2.5106 - val_acc: 0.3666\n",
      "Epoch 58/60\n",
      "450000/450000 [==============================] - 204s - loss: 2.5177 - acc: 0.3428 - val_loss: 2.3794 - val_acc: 0.3922\n",
      "Epoch 59/60\n",
      "450000/450000 [==============================] - 204s - loss: 2.5162 - acc: 0.3440 - val_loss: 2.3751 - val_acc: 0.3800\n",
      "Epoch 60/60\n",
      "450000/450000 [==============================] - 204s - loss: 2.5077 - acc: 0.3452 - val_loss: 2.2717 - val_acc: 0.4034\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 61/65\n",
      "450000/450000 [==============================] - 204s - loss: 2.5070 - acc: 0.3458 - val_loss: 2.3868 - val_acc: 0.3854\n",
      "Epoch 62/65\n",
      "450000/450000 [==============================] - 204s - loss: 2.5073 - acc: 0.3459 - val_loss: 2.3019 - val_acc: 0.3958\n",
      "Epoch 63/65\n",
      "450000/450000 [==============================] - 204s - loss: 2.5050 - acc: 0.3468 - val_loss: 2.3174 - val_acc: 0.3960\n",
      "Epoch 64/65\n",
      "450000/450000 [==============================] - 204s - loss: 2.5022 - acc: 0.3471 - val_loss: 2.2665 - val_acc: 0.4040\n",
      "Epoch 65/65\n",
      "450000/450000 [==============================] - 204s - loss: 2.4953 - acc: 0.3488 - val_loss: 2.4001 - val_acc: 0.3760\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 66/70\n",
      "450000/450000 [==============================] - 204s - loss: 2.4946 - acc: 0.3477 - val_loss: 2.3239 - val_acc: 0.3996\n",
      "Epoch 67/70\n",
      "450000/450000 [==============================] - 204s - loss: 2.4938 - acc: 0.3488 - val_loss: 2.3110 - val_acc: 0.4016\n",
      "Epoch 68/70\n",
      "450000/450000 [==============================] - 204s - loss: 2.4875 - acc: 0.3499 - val_loss: 2.3538 - val_acc: 0.3918\n",
      "Epoch 69/70\n",
      "450000/450000 [==============================] - 204s - loss: 2.4858 - acc: 0.3504 - val_loss: 2.3850 - val_acc: 0.3910\n",
      "Epoch 70/70\n",
      "450000/450000 [==============================] - 204s - loss: 2.4849 - acc: 0.3504 - val_loss: 2.3684 - val_acc: 0.3900\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 71/75\n",
      "450000/450000 [==============================] - 204s - loss: 2.4824 - acc: 0.3508 - val_loss: 2.2694 - val_acc: 0.4102\n",
      "Epoch 72/75\n",
      "450000/450000 [==============================] - 204s - loss: 2.4804 - acc: 0.3514 - val_loss: 2.3570 - val_acc: 0.3940\n",
      "Epoch 73/75\n",
      "450000/450000 [==============================] - 204s - loss: 2.4753 - acc: 0.3518 - val_loss: 2.3871 - val_acc: 0.3904\n",
      "Epoch 74/75\n",
      "450000/450000 [==============================] - 205s - loss: 2.4694 - acc: 0.3538 - val_loss: 2.4188 - val_acc: 0.3696\n",
      "Epoch 75/75\n",
      "450000/450000 [==============================] - 205s - loss: 2.4682 - acc: 0.3543 - val_loss: 2.3061 - val_acc: 0.4022\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 76/80\n",
      "450000/450000 [==============================] - 205s - loss: 2.4662 - acc: 0.3538 - val_loss: 2.2951 - val_acc: 0.3970\n",
      "Epoch 77/80\n",
      "450000/450000 [==============================] - 204s - loss: 2.4604 - acc: 0.3547 - val_loss: 2.2763 - val_acc: 0.4138\n",
      "Epoch 78/80\n",
      "450000/450000 [==============================] - 204s - loss: 2.4601 - acc: 0.3553 - val_loss: 2.3031 - val_acc: 0.3978\n",
      "Epoch 79/80\n",
      "450000/450000 [==============================] - 204s - loss: 2.4601 - acc: 0.3551 - val_loss: 2.2912 - val_acc: 0.4078\n",
      "Epoch 80/80\n",
      "450000/450000 [==============================] - 204s - loss: 2.4548 - acc: 0.3566 - val_loss: 2.3078 - val_acc: 0.3976\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 81/85\n",
      "450000/450000 [==============================] - 204s - loss: 2.4521 - acc: 0.3582 - val_loss: 2.3214 - val_acc: 0.4028\n",
      "Epoch 82/85\n",
      "450000/450000 [==============================] - 205s - loss: 2.4504 - acc: 0.3578 - val_loss: 2.2119 - val_acc: 0.4184\n",
      "Epoch 83/85\n",
      "450000/450000 [==============================] - 205s - loss: 2.4484 - acc: 0.3582 - val_loss: 2.2759 - val_acc: 0.4140\n",
      "Epoch 84/85\n",
      "450000/450000 [==============================] - 205s - loss: 2.4434 - acc: 0.3592 - val_loss: 2.2903 - val_acc: 0.3940\n",
      "Epoch 85/85\n",
      "450000/450000 [==============================] - 205s - loss: 2.4462 - acc: 0.3590 - val_loss: 2.3370 - val_acc: 0.4012\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 86/90\n",
      "450000/450000 [==============================] - 204s - loss: 2.4441 - acc: 0.3592 - val_loss: 2.4100 - val_acc: 0.3856\n",
      "Epoch 87/90\n",
      "450000/450000 [==============================] - 204s - loss: 2.4382 - acc: 0.3602 - val_loss: 2.2139 - val_acc: 0.4184\n",
      "Epoch 88/90\n",
      "450000/450000 [==============================] - 204s - loss: 2.4368 - acc: 0.3608 - val_loss: 2.2605 - val_acc: 0.4086\n",
      "Epoch 89/90\n",
      "450000/450000 [==============================] - 204s - loss: 2.4360 - acc: 0.3607 - val_loss: 2.3716 - val_acc: 0.3988\n",
      "Epoch 90/90\n",
      "450000/450000 [==============================] - 204s - loss: 2.4361 - acc: 0.3608 - val_loss: 2.3708 - val_acc: 0.3908\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 91/95\n",
      "450000/450000 [==============================] - 204s - loss: 2.4313 - acc: 0.3620 - val_loss: 2.2581 - val_acc: 0.4084\n",
      "Epoch 92/95\n",
      "450000/450000 [==============================] - 204s - loss: 2.4271 - acc: 0.3631 - val_loss: 2.3093 - val_acc: 0.4040\n",
      "Epoch 93/95\n",
      "450000/450000 [==============================] - 204s - loss: 2.4232 - acc: 0.3632 - val_loss: 2.2626 - val_acc: 0.4080\n",
      "Epoch 94/95\n",
      "450000/450000 [==============================] - 204s - loss: 2.4199 - acc: 0.3643 - val_loss: 2.2328 - val_acc: 0.4160\n",
      "Epoch 95/95\n",
      "450000/450000 [==============================] - 204s - loss: 2.4214 - acc: 0.3636 - val_loss: 2.2256 - val_acc: 0.4200\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 96/100\n",
      "450000/450000 [==============================] - 204s - loss: 2.4189 - acc: 0.3655 - val_loss: 2.2769 - val_acc: 0.4084\n",
      "Epoch 97/100\n",
      "450000/450000 [==============================] - 204s - loss: 2.4138 - acc: 0.3660 - val_loss: 2.1752 - val_acc: 0.4300\n",
      "Epoch 98/100\n",
      "450000/450000 [==============================] - 204s - loss: 2.4155 - acc: 0.3658 - val_loss: 2.2250 - val_acc: 0.4150\n",
      "Epoch 99/100\n",
      "450000/450000 [==============================] - 204s - loss: 2.4112 - acc: 0.3670 - val_loss: 2.2251 - val_acc: 0.4166\n",
      "Epoch 100/100\n",
      "450000/450000 [==============================] - 204s - loss: 2.4113 - acc: 0.3665 - val_loss: 2.2772 - val_acc: 0.4176\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 101/105\n",
      "450000/450000 [==============================] - 204s - loss: 2.4090 - acc: 0.3678 - val_loss: 2.3448 - val_acc: 0.4028\n",
      "Epoch 102/105\n",
      "450000/450000 [==============================] - 204s - loss: 2.4086 - acc: 0.3680 - val_loss: 2.1960 - val_acc: 0.4266\n",
      "Epoch 103/105\n",
      "450000/450000 [==============================] - 204s - loss: 2.4058 - acc: 0.3679 - val_loss: 2.2631 - val_acc: 0.4106\n",
      "Epoch 104/105\n",
      "450000/450000 [==============================] - 204s - loss: 2.4004 - acc: 0.3692 - val_loss: 2.2723 - val_acc: 0.4126\n",
      "Epoch 105/105\n",
      "450000/450000 [==============================] - 204s - loss: 2.3964 - acc: 0.3695 - val_loss: 2.2074 - val_acc: 0.4280\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 106/110\n",
      "450000/450000 [==============================] - 204s - loss: 2.3941 - acc: 0.3705 - val_loss: 2.2233 - val_acc: 0.4218\n",
      "Epoch 107/110\n",
      "450000/450000 [==============================] - 204s - loss: 2.3968 - acc: 0.3700 - val_loss: 2.3113 - val_acc: 0.4176\n",
      "Epoch 108/110\n",
      "450000/450000 [==============================] - 204s - loss: 2.3839 - acc: 0.3729 - val_loss: 2.2005 - val_acc: 0.4298\n",
      "Epoch 109/110\n",
      "450000/450000 [==============================] - 204s - loss: 2.3881 - acc: 0.3719 - val_loss: 2.2530 - val_acc: 0.4086\n",
      "Epoch 110/110\n",
      "450000/450000 [==============================] - 204s - loss: 2.3825 - acc: 0.3742 - val_loss: 2.2114 - val_acc: 0.4226\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 111/115\n",
      "450000/450000 [==============================] - 204s - loss: 2.3814 - acc: 0.3734 - val_loss: 2.2464 - val_acc: 0.4186\n",
      "Epoch 112/115\n",
      "450000/450000 [==============================] - 204s - loss: 2.3808 - acc: 0.3744 - val_loss: 2.2876 - val_acc: 0.4238\n",
      "Epoch 113/115\n",
      "450000/450000 [==============================] - 204s - loss: 2.3836 - acc: 0.3728 - val_loss: 2.2055 - val_acc: 0.4264\n",
      "Epoch 114/115\n",
      "450000/450000 [==============================] - 204s - loss: 2.3775 - acc: 0.3749 - val_loss: 2.2022 - val_acc: 0.4362\n",
      "Epoch 115/115\n",
      "450000/450000 [==============================] - 204s - loss: 2.3711 - acc: 0.3764 - val_loss: 2.2247 - val_acc: 0.4242\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 116/120\n",
      "450000/450000 [==============================] - 204s - loss: 2.3649 - acc: 0.3780 - val_loss: 2.2763 - val_acc: 0.4186\n",
      "Epoch 117/120\n",
      "450000/450000 [==============================] - 202s - loss: 2.3671 - acc: 0.3779 - val_loss: 2.1927 - val_acc: 0.4288\n",
      "Epoch 118/120\n",
      "450000/450000 [==============================] - 202s - loss: 2.3658 - acc: 0.3780 - val_loss: 2.2980 - val_acc: 0.4118\n",
      "Epoch 119/120\n",
      "450000/450000 [==============================] - 203s - loss: 2.3633 - acc: 0.3785 - val_loss: 2.2648 - val_acc: 0.4192\n",
      "Epoch 120/120\n",
      "450000/450000 [==============================] - 204s - loss: 2.3594 - acc: 0.3795 - val_loss: 2.2252 - val_acc: 0.4360\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 121/125\n",
      "450000/450000 [==============================] - 204s - loss: 2.3591 - acc: 0.3798 - val_loss: 2.2141 - val_acc: 0.4250\n",
      "Epoch 122/125\n",
      "450000/450000 [==============================] - 204s - loss: 2.3570 - acc: 0.3815 - val_loss: 2.1704 - val_acc: 0.4350\n",
      "Epoch 123/125\n",
      "450000/450000 [==============================] - 204s - loss: 2.3557 - acc: 0.3813 - val_loss: 2.1555 - val_acc: 0.4396\n",
      "Epoch 124/125\n",
      "450000/450000 [==============================] - 204s - loss: 2.3511 - acc: 0.3816 - val_loss: 2.2366 - val_acc: 0.4344\n",
      "Epoch 125/125\n",
      "450000/450000 [==============================] - 204s - loss: 2.3477 - acc: 0.3830 - val_loss: 2.1855 - val_acc: 0.4258\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 126/130\n",
      "450000/450000 [==============================] - 204s - loss: 2.3465 - acc: 0.3831 - val_loss: 2.2500 - val_acc: 0.4204\n",
      "Epoch 127/130\n",
      "450000/450000 [==============================] - 204s - loss: 2.3450 - acc: 0.3840 - val_loss: 2.0803 - val_acc: 0.4512\n",
      "Epoch 128/130\n",
      "450000/450000 [==============================] - 204s - loss: 2.3376 - acc: 0.3853 - val_loss: 2.1730 - val_acc: 0.4330\n",
      "Epoch 129/130\n",
      "450000/450000 [==============================] - 204s - loss: 2.3348 - acc: 0.3862 - val_loss: 2.1331 - val_acc: 0.4434\n",
      "Epoch 130/130\n",
      "450000/450000 [==============================] - 204s - loss: 2.3307 - acc: 0.3876 - val_loss: 2.1981 - val_acc: 0.4394\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 131/135\n",
      "450000/450000 [==============================] - 204s - loss: 2.3330 - acc: 0.3874 - val_loss: 2.1792 - val_acc: 0.4382\n",
      "Epoch 132/135\n",
      "450000/450000 [==============================] - 204s - loss: 2.3298 - acc: 0.3876 - val_loss: 2.1661 - val_acc: 0.4436\n",
      "Epoch 133/135\n",
      "450000/450000 [==============================] - 204s - loss: 2.3315 - acc: 0.3876 - val_loss: 2.1854 - val_acc: 0.4440\n",
      "Epoch 134/135\n",
      "450000/450000 [==============================] - 204s - loss: 2.3289 - acc: 0.3883 - val_loss: 2.2874 - val_acc: 0.4218\n",
      "Epoch 135/135\n",
      "450000/450000 [==============================] - 204s - loss: 2.3239 - acc: 0.3902 - val_loss: 2.2374 - val_acc: 0.4360\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 136/140\n",
      "450000/450000 [==============================] - 204s - loss: 2.3189 - acc: 0.3918 - val_loss: 2.2478 - val_acc: 0.4296\n",
      "Epoch 137/140\n",
      "450000/450000 [==============================] - 204s - loss: 2.3165 - acc: 0.3908 - val_loss: 2.1710 - val_acc: 0.4470\n",
      "Epoch 138/140\n",
      "450000/450000 [==============================] - 204s - loss: 2.3152 - acc: 0.3921 - val_loss: 2.1462 - val_acc: 0.4424\n",
      "Epoch 139/140\n",
      "450000/450000 [==============================] - 204s - loss: 2.3141 - acc: 0.3931 - val_loss: 2.0847 - val_acc: 0.4598\n",
      "Epoch 140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000/450000 [==============================] - 204s - loss: 2.3089 - acc: 0.3946 - val_loss: 2.2341 - val_acc: 0.4490\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 141/145\n",
      "450000/450000 [==============================] - 204s - loss: 2.3108 - acc: 0.3946 - val_loss: 2.2022 - val_acc: 0.4528\n",
      "Epoch 142/145\n",
      "450000/450000 [==============================] - 204s - loss: 2.3084 - acc: 0.3940 - val_loss: 2.1241 - val_acc: 0.4430\n",
      "Epoch 143/145\n",
      "450000/450000 [==============================] - 204s - loss: 2.3074 - acc: 0.3954 - val_loss: 2.1306 - val_acc: 0.4542\n",
      "Epoch 144/145\n",
      "450000/450000 [==============================] - 204s - loss: 2.3054 - acc: 0.3962 - val_loss: 2.1488 - val_acc: 0.4448\n",
      "Epoch 145/145\n",
      "450000/450000 [==============================] - 204s - loss: 2.3017 - acc: 0.3977 - val_loss: 2.1197 - val_acc: 0.4490\n",
      "Train on 450000 samples, validate on 5000 samples\n",
      "Epoch 146/150\n",
      "450000/450000 [==============================] - 204s - loss: 2.3048 - acc: 0.3963 - val_loss: 2.1221 - val_acc: 0.4514\n",
      "Epoch 147/150\n",
      "450000/450000 [==============================] - 204s - loss: 2.3000 - acc: 0.3973 - val_loss: 2.1662 - val_acc: 0.4512\n",
      "Epoch 148/150\n",
      "450000/450000 [==============================] - 204s - loss: 2.2998 - acc: 0.3979 - val_loss: 2.1096 - val_acc: 0.4640\n",
      "Epoch 149/150\n",
      "450000/450000 [==============================] - 204s - loss: 2.2989 - acc: 0.3995 - val_loss: 2.1756 - val_acc: 0.4516\n",
      "Epoch 150/150\n",
      "450000/450000 [==============================] - 204s - loss: 2.2955 - acc: 0.3994 - val_loss: 2.1439 - val_acc: 0.4552\n"
     ]
    }
   ],
   "source": [
    "index= 25\n",
    "step = 5\n",
    "stop = 150\n",
    "\n",
    "while index < stop:\n",
    "    base_model.fit(x_train, y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "                   validation_data=(x_val, y_val), callbacks=[tbCallBack])\n",
    "    index += step\n",
    "    base_model.save_weights('data/models/base_model_'+str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define New Thresholding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "##### THRESHOLDING LAYER IS NOT FUNCTIONAL #####\n",
    "class Threshold(Layer):\n",
    "    \"\"\"Thresholded Rectified Linear Unit.\n",
    "    This layer takes in a set of coarse probabilities and an\n",
    "    index pointing to the relevant coarse probability\n",
    "    \n",
    "    It then thresholds that probability following:\n",
    "    `f(x) = 1 for x > theta`,\n",
    "    `f(x) = 0 otherwise`.\n",
    "    \n",
    "    Finally it multiplies that thresholded value across the \n",
    "    full input layer.\n",
    "    \n",
    "    # Input shape\n",
    "        Arbitrary 4-D Tensor\n",
    "    # Output\n",
    "        An tensor with the same shape as the input but with values\n",
    "        that are either all 1s or all 0s\n",
    "    # Arguments\n",
    "        theta: float >= 0. Threshold value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, index, theta=.5, **kwargs):\n",
    "        super(Threshold, self).__init__(**kwargs)\n",
    "        self.theta = K.cast_to_floatx(theta)\n",
    "        self.index = index\n",
    "        self.prob = prob\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Get Shape of Input\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        # Pull out coarse probability as specified by self.prob\n",
    "        X = self.prob[:,self.index:self.index+1]\n",
    "        # Threshold probability\n",
    "        X = K.cast(K.greater(X, self.theta), K.floatx())\n",
    "        # Repeat and Reshape thresholded probability to match input\n",
    "        X = K.tile(X, [1, shape[1]*shape[2]*shape[3]])\n",
    "        X = K.reshape(X, [-1,shape[1],shape[2],shape[3]])\n",
    "        # Multiply input with thresholded probability\n",
    "        X = Multiply()([X,inputs])\n",
    "        return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'theta': float(self.theta)}\n",
    "        base_config = super(Threshold, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Fine Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Calculate Probabalistic Averaging Layer\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function calculates the coarse classifier probabilities and\n",
    "#        extends that to a probabalistic averaging over fine classifications\n",
    "#    \n",
    "#    Parameters:\n",
    "#        X      The output of the coarse classifier\n",
    "# \n",
    "#    Returns:\n",
    "#        Coarse category predictions and probabalistic averaging layer\n",
    "################################################################################\n",
    "def calc_prob_ave(X):\n",
    "    # Coarse to Fine mapping\n",
    "    ck = Input(shape=(20, 100), dtype='float32', name='C2K')\n",
    "    \n",
    "    # Extend input dimension from (n,100) to (n,1,100)\n",
    "    X = RepeatVector(1)(X)\n",
    "    \n",
    "    # Dot input, (n,1,100) and coarse to fine mapping (n,20,100) on axis 2\n",
    "    X = Dot(axes=2)([ck,X])\n",
    "    # Flatten result to (n,20)\n",
    "    Y = Reshape([20])(X)\n",
    "    \n",
    "    # Dot the vectors again to get probabalistic averaging layer (n,20,100)*(n,20,1) = (n,1,100)\n",
    "    X = Dot(axes=1)([ck,X])\n",
    "    # Flatten the result to (n,100)\n",
    "    X = Reshape([100])(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate probabalistic averaging layer and coarse probabilities\n",
    "prob_ave, coarse_prob = calc_prob_ave(output_c, ck)\n",
    "\n",
    "# Create new independent layers for each coarse category\n",
    "for i in range(20):\n",
    "    #thresholded = Threshold(coarse_prob,i)(net_shared) - Threshold layer not functional\n",
    "    net = indep_layers(net_shared, str(i+1))\n",
    "    fine_output = Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    if i==0: output_f = fine_output\n",
    "    else: output_f = Concatenate(axis=1)([output_f, fine_output])\n",
    "\n",
    "\n",
    "# Apply probabalistic averaging layer to the result of the fine predictions\n",
    "weighted = Multiply()([output_f, prob_ave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile new network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[base_model.input, ck], \n",
    "              outputs=weighted)\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse-To-Fine Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually will want to do spectral clustering for coarse to fine mappings\n",
    "# currently just using coarse categories defined by cifar\n",
    "\n",
    "##### REPLACE WITH PRE_BUILT ONE HOT ENCODER #####\n",
    "# Get Coarse to Fine Mappings\n",
    "C2K = np.loadtxt('C2K.txt', dtype=int)\n",
    "C2K_dot = np.zeros((20,100))\n",
    "for i in range(len(C2K)):\n",
    "    C2K_dot[C2K[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### REPLACE WITH NP.REPEAT #####\n",
    "C2K_train = np.zeros((len(x_train), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_val = np.zeros((len(x_val), len(C2K_dot), len(C2K_dot[0])))\n",
    "C2K_test = np.zeros((len(x_test), len(C2K_dot), len(C2K_dot[0])))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    C2K_train[i] = C2K_dot\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    C2K_train[i] = C2K_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Coarse Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    # Save layer name for readability\n",
    "    layer_name = layer.get_config()['name']\n",
    "    \n",
    "    # Set all shared layers to not be trainable\n",
    "    if 'shared_layer' in layer_name:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Load weights for new independent layers\n",
    "    if 'indep_layer' in layer_name:\n",
    "        seg = layer_name.split('__')[1]\n",
    "        for f_layer in model.layers:\n",
    "            if seg in f_layer.get_config()['name']:\n",
    "                f_layer.set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 5\n",
    "stop = 50\n",
    "\n",
    "while index < stop:\n",
    "    model.fit([x_train, C2K_train], y_train, initial_epoch=index, epochs=index+step, batch_size=256, \\\n",
    "             validation_data=([x_val, C2K_val],y_val))\n",
    "    model.save_weights('data/models/layer_'+str(index))\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = np.zeros((20,len(x_test),100))\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(20):\n",
    "        ct[j,i] = C2K_dot[j]\n",
    "        \n",
    "model.evaluate([x_test, C2K_test], y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
